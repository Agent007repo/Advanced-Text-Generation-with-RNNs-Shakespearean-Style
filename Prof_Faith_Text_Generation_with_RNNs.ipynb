{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install torch numpy tokenizers nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbFl4aIBN2rZ",
        "outputId": "60a48d7a-a83a-491c-cca8-56bb8f62dbf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.29.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6cgsterTM_I5",
        "outputId": "3b1e0cb6-f438-4dfd-fc07-8079dfb878d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed.\n",
            "Checking/downloading NLTK data (punkt, wordnet, omw-1.4)...\n",
            "---------------------------------------------------------\n",
            "NLTK data download might be needed or failed previously.\n",
            "If stemming fails later, run the following in a code cell:\n",
            "import nltk; nltk.download('punkt'); nltk.download('wordnet'); nltk.download('omw-1.4')\n",
            "---------------------------------------------------------\n",
            "Using device: cuda\n",
            "Tokenizer will be saved/loaded from: shakespeare_tokenizer_bpe_8000.json\n",
            "Best model will be saved to: best_model_shakespeare_lstm.pt\n",
            "Please upload the dataset file ('shakespeare.txt' or specify different name in Config)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b30511c5-fbd9-428c-8307-2765bd4a7eb5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b30511c5-fbd9-428c-8307-2765bd4a7eb5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving shakespeare.txt to shakespeare (1).txt\n",
            "Assuming uploaded file 'shakespeare (1).txt' is the dataset.\n",
            "Successfully loaded 1155394 characters from shakespeare (1).txt\n",
            "Loading existing tokenizer from shakespeare_tokenizer_bpe_8000.json\n",
            "Processing text for dataset...\n",
            "Encoding text with tokenizer...\n",
            "Encoding complete.\n",
            "Creating sequences...\n",
            "Sequence creation complete.\n",
            "\n",
            "Tokenizer Vocabulary size: 8000\n",
            "Total tokens in processed text: 275814\n",
            "Total sequences created: 275754\n",
            "Training set size: 248179\n",
            "Validation set size: 27575\n",
            "Number of training batches: 1938\n",
            "Number of validation batches: 215\n",
            "\n",
            "--- Model Architecture ---\n",
            "RNNModel(\n",
            "  (embedding): Embedding(8000, 256)\n",
            "  (rnn): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout_layer): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=8000, bias=True)\n",
            ")\n",
            "Total trainable parameters: 9,830,208\n",
            "No pre-trained model specified or found. Training from scratch.\n",
            "\n",
            "--- Starting Training ---\n",
            "| epoch   1 |    50/ 1938 batches | ms/batch 136.45 | loss  7.40 | ppl  1635.98\n",
            "| epoch   1 |   100/ 1938 batches | ms/batch 100.44 | loss  6.57 | ppl   715.91\n",
            "| epoch   1 |   150/ 1938 batches | ms/batch 101.07 | loss  6.33 | ppl   561.64\n",
            "| epoch   1 |   200/ 1938 batches | ms/batch 101.20 | loss  6.13 | ppl   457.29\n",
            "| epoch   1 |   250/ 1938 batches | ms/batch 101.26 | loss  5.86 | ppl   349.19\n",
            "| epoch   1 |   300/ 1938 batches | ms/batch 101.23 | loss  5.66 | ppl   288.25\n",
            "| epoch   1 |   350/ 1938 batches | ms/batch 100.88 | loss  5.53 | ppl   252.24\n",
            "| epoch   1 |   400/ 1938 batches | ms/batch 101.24 | loss  5.41 | ppl   224.41\n",
            "| epoch   1 |   450/ 1938 batches | ms/batch 101.32 | loss  5.32 | ppl   205.05\n",
            "| epoch   1 |   500/ 1938 batches | ms/batch 101.39 | loss  5.23 | ppl   186.48\n",
            "| epoch   1 |   550/ 1938 batches | ms/batch 102.62 | loss  5.13 | ppl   168.38\n",
            "| epoch   1 |   600/ 1938 batches | ms/batch 101.42 | loss  5.05 | ppl   156.51\n",
            "| epoch   1 |   650/ 1938 batches | ms/batch 101.19 | loss  4.94 | ppl   140.23\n",
            "| epoch   1 |   700/ 1938 batches | ms/batch 101.75 | loss  4.86 | ppl   129.25\n",
            "| epoch   1 |   750/ 1938 batches | ms/batch 101.20 | loss  4.79 | ppl   120.28\n",
            "| epoch   1 |   800/ 1938 batches | ms/batch 102.25 | loss  4.71 | ppl   111.13\n",
            "| epoch   1 |   850/ 1938 batches | ms/batch 103.39 | loss  4.64 | ppl   103.56\n",
            "| epoch   1 |   900/ 1938 batches | ms/batch 103.00 | loss  4.58 | ppl    97.65\n",
            "| epoch   1 |   950/ 1938 batches | ms/batch 104.14 | loss  4.51 | ppl    90.49\n",
            "| epoch   1 |  1000/ 1938 batches | ms/batch 108.70 | loss  4.44 | ppl    85.10\n",
            "| epoch   1 |  1050/ 1938 batches | ms/batch 112.12 | loss  4.38 | ppl    80.04\n",
            "| epoch   1 |  1100/ 1938 batches | ms/batch 104.95 | loss  4.32 | ppl    75.07\n",
            "| epoch   1 |  1150/ 1938 batches | ms/batch 105.79 | loss  4.29 | ppl    72.97\n",
            "| epoch   1 |  1200/ 1938 batches | ms/batch 106.28 | loss  4.22 | ppl    67.93\n",
            "| epoch   1 |  1250/ 1938 batches | ms/batch 106.22 | loss  4.18 | ppl    65.20\n",
            "| epoch   1 |  1300/ 1938 batches | ms/batch 106.99 | loss  4.12 | ppl    61.67\n",
            "| epoch   1 |  1350/ 1938 batches | ms/batch 106.67 | loss  4.08 | ppl    59.40\n",
            "| epoch   1 |  1400/ 1938 batches | ms/batch 107.42 | loss  4.04 | ppl    57.10\n",
            "| epoch   1 |  1450/ 1938 batches | ms/batch 107.45 | loss  4.00 | ppl    54.84\n",
            "| epoch   1 |  1500/ 1938 batches | ms/batch 108.48 | loss  3.97 | ppl    52.76\n",
            "| epoch   1 |  1550/ 1938 batches | ms/batch 108.35 | loss  3.94 | ppl    51.29\n",
            "| epoch   1 |  1600/ 1938 batches | ms/batch 108.86 | loss  3.89 | ppl    48.97\n",
            "| epoch   1 |  1650/ 1938 batches | ms/batch 109.31 | loss  3.86 | ppl    47.51\n",
            "| epoch   1 |  1700/ 1938 batches | ms/batch 109.34 | loss  3.83 | ppl    46.19\n",
            "| epoch   1 |  1750/ 1938 batches | ms/batch 110.23 | loss  3.81 | ppl    45.07\n",
            "| epoch   1 |  1800/ 1938 batches | ms/batch 109.86 | loss  3.78 | ppl    43.66\n",
            "| epoch   1 |  1850/ 1938 batches | ms/batch 110.96 | loss  3.75 | ppl    42.63\n",
            "| epoch   1 |  1900/ 1938 batches | ms/batch 109.59 | loss  3.73 | ppl    41.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 292.14s | train loss  3.05 | train ppl    21.14 | valid loss  3.08 | valid ppl    21.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 1) to best_model_shakespeare_lstm.pt with Val PPL: 21.71 **\n",
            "| epoch   2 |    50/ 1938 batches | ms/batch 127.90 | loss  3.73 | ppl    41.79\n",
            "| epoch   2 |   100/ 1938 batches | ms/batch 112.02 | loss  3.64 | ppl    38.00\n",
            "| epoch   2 |   150/ 1938 batches | ms/batch 111.66 | loss  3.63 | ppl    37.78\n",
            "| epoch   2 |   200/ 1938 batches | ms/batch 112.56 | loss  3.60 | ppl    36.59\n",
            "| epoch   2 |   250/ 1938 batches | ms/batch 112.38 | loss  3.58 | ppl    35.91\n",
            "| epoch   2 |   300/ 1938 batches | ms/batch 113.31 | loss  3.56 | ppl    35.30\n",
            "| epoch   2 |   350/ 1938 batches | ms/batch 112.71 | loss  3.55 | ppl    34.71\n",
            "| epoch   2 |   400/ 1938 batches | ms/batch 113.29 | loss  3.53 | ppl    34.08\n",
            "| epoch   2 |   450/ 1938 batches | ms/batch 112.87 | loss  3.52 | ppl    33.69\n",
            "| epoch   2 |   500/ 1938 batches | ms/batch 112.94 | loss  3.51 | ppl    33.29\n",
            "| epoch   2 |   550/ 1938 batches | ms/batch 113.11 | loss  3.48 | ppl    32.41\n",
            "| epoch   2 |   600/ 1938 batches | ms/batch 113.12 | loss  3.47 | ppl    32.12\n",
            "| epoch   2 |   650/ 1938 batches | ms/batch 113.71 | loss  3.45 | ppl    31.46\n",
            "| epoch   2 |   700/ 1938 batches | ms/batch 113.69 | loss  3.44 | ppl    31.09\n",
            "| epoch   2 |   750/ 1938 batches | ms/batch 113.88 | loss  3.43 | ppl    30.73\n",
            "| epoch   2 |   800/ 1938 batches | ms/batch 113.61 | loss  3.41 | ppl    30.36\n",
            "| epoch   2 |   850/ 1938 batches | ms/batch 113.93 | loss  3.41 | ppl    30.19\n",
            "| epoch   2 |   900/ 1938 batches | ms/batch 113.81 | loss  3.39 | ppl    29.59\n",
            "| epoch   2 |   950/ 1938 batches | ms/batch 114.25 | loss  3.38 | ppl    29.32\n",
            "| epoch   2 |  1000/ 1938 batches | ms/batch 114.08 | loss  3.37 | ppl    28.96\n",
            "| epoch   2 |  1050/ 1938 batches | ms/batch 114.40 | loss  3.36 | ppl    28.70\n",
            "| epoch   2 |  1100/ 1938 batches | ms/batch 114.05 | loss  3.34 | ppl    28.33\n",
            "| epoch   2 |  1150/ 1938 batches | ms/batch 114.27 | loss  3.33 | ppl    28.05\n",
            "| epoch   2 |  1200/ 1938 batches | ms/batch 114.02 | loss  3.32 | ppl    27.65\n",
            "| epoch   2 |  1250/ 1938 batches | ms/batch 114.50 | loss  3.31 | ppl    27.37\n",
            "| epoch   2 |  1300/ 1938 batches | ms/batch 114.23 | loss  3.30 | ppl    27.09\n",
            "| epoch   2 |  1350/ 1938 batches | ms/batch 114.47 | loss  3.28 | ppl    26.68\n",
            "| epoch   2 |  1400/ 1938 batches | ms/batch 114.44 | loss  3.27 | ppl    26.37\n",
            "| epoch   2 |  1450/ 1938 batches | ms/batch 114.57 | loss  3.27 | ppl    26.41\n",
            "| epoch   2 |  1500/ 1938 batches | ms/batch 114.66 | loss  3.27 | ppl    26.31\n",
            "| epoch   2 |  1550/ 1938 batches | ms/batch 114.72 | loss  3.26 | ppl    26.06\n",
            "| epoch   2 |  1600/ 1938 batches | ms/batch 115.31 | loss  3.25 | ppl    25.72\n",
            "| epoch   2 |  1650/ 1938 batches | ms/batch 114.84 | loss  3.24 | ppl    25.43\n",
            "| epoch   2 |  1700/ 1938 batches | ms/batch 115.11 | loss  3.23 | ppl    25.39\n",
            "| epoch   2 |  1750/ 1938 batches | ms/batch 114.86 | loss  3.22 | ppl    25.15\n",
            "| epoch   2 |  1800/ 1938 batches | ms/batch 115.02 | loss  3.21 | ppl    24.88\n",
            "| epoch   2 |  1850/ 1938 batches | ms/batch 114.95 | loss  3.21 | ppl    24.77\n",
            "| epoch   2 |  1900/ 1938 batches | ms/batch 115.49 | loss  3.20 | ppl    24.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 309.63s | train loss  2.31 | train ppl    10.04 | valid loss  2.33 | valid ppl    10.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 2) to best_model_shakespeare_lstm.pt with Val PPL: 10.32 **\n",
            "| epoch   3 |    50/ 1938 batches | ms/batch 136.28 | loss  3.23 | ppl    25.31\n",
            "| epoch   3 |   100/ 1938 batches | ms/batch 114.58 | loss  3.16 | ppl    23.67\n",
            "| epoch   3 |   150/ 1938 batches | ms/batch 114.98 | loss  3.16 | ppl    23.52\n",
            "| epoch   3 |   200/ 1938 batches | ms/batch 114.78 | loss  3.15 | ppl    23.30\n",
            "| epoch   3 |   250/ 1938 batches | ms/batch 114.73 | loss  3.15 | ppl    23.33\n",
            "| epoch   3 |   300/ 1938 batches | ms/batch 114.85 | loss  3.14 | ppl    23.16\n",
            "| epoch   3 |   350/ 1938 batches | ms/batch 114.98 | loss  3.14 | ppl    23.16\n",
            "| epoch   3 |   400/ 1938 batches | ms/batch 114.93 | loss  3.13 | ppl    22.90\n",
            "| epoch   3 |   450/ 1938 batches | ms/batch 115.19 | loss  3.12 | ppl    22.69\n",
            "| epoch   3 |   500/ 1938 batches | ms/batch 114.94 | loss  3.13 | ppl    22.80\n",
            "| epoch   3 |   550/ 1938 batches | ms/batch 115.32 | loss  3.12 | ppl    22.71\n",
            "| epoch   3 |   600/ 1938 batches | ms/batch 114.85 | loss  3.11 | ppl    22.36\n",
            "| epoch   3 |   650/ 1938 batches | ms/batch 115.28 | loss  3.11 | ppl    22.33\n",
            "| epoch   3 |   700/ 1938 batches | ms/batch 115.28 | loss  3.10 | ppl    22.17\n",
            "| epoch   3 |   750/ 1938 batches | ms/batch 115.29 | loss  3.09 | ppl    22.06\n",
            "| epoch   3 |   800/ 1938 batches | ms/batch 115.28 | loss  3.09 | ppl    22.01\n",
            "| epoch   3 |   850/ 1938 batches | ms/batch 115.13 | loss  3.09 | ppl    21.96\n",
            "| epoch   3 |   900/ 1938 batches | ms/batch 115.05 | loss  3.09 | ppl    21.89\n",
            "| epoch   3 |   950/ 1938 batches | ms/batch 115.44 | loss  3.08 | ppl    21.66\n",
            "| epoch   3 |  1000/ 1938 batches | ms/batch 115.27 | loss  3.07 | ppl    21.46\n",
            "| epoch   3 |  1050/ 1938 batches | ms/batch 115.53 | loss  3.07 | ppl    21.60\n",
            "| epoch   3 |  1100/ 1938 batches | ms/batch 115.44 | loss  3.06 | ppl    21.38\n",
            "| epoch   3 |  1150/ 1938 batches | ms/batch 115.91 | loss  3.06 | ppl    21.38\n",
            "| epoch   3 |  1200/ 1938 batches | ms/batch 115.62 | loss  3.04 | ppl    20.96\n",
            "| epoch   3 |  1250/ 1938 batches | ms/batch 115.45 | loss  3.04 | ppl    20.98\n",
            "| epoch   3 |  1300/ 1938 batches | ms/batch 115.67 | loss  3.04 | ppl    20.92\n",
            "| epoch   3 |  1350/ 1938 batches | ms/batch 115.41 | loss  3.04 | ppl    20.97\n",
            "| epoch   3 |  1400/ 1938 batches | ms/batch 115.02 | loss  3.03 | ppl    20.73\n",
            "| epoch   3 |  1450/ 1938 batches | ms/batch 114.74 | loss  3.03 | ppl    20.78\n",
            "| epoch   3 |  1500/ 1938 batches | ms/batch 114.68 | loss  3.03 | ppl    20.69\n",
            "| epoch   3 |  1550/ 1938 batches | ms/batch 114.47 | loss  3.02 | ppl    20.57\n",
            "| epoch   3 |  1600/ 1938 batches | ms/batch 114.52 | loss  3.02 | ppl    20.49\n",
            "| epoch   3 |  1650/ 1938 batches | ms/batch 114.30 | loss  3.02 | ppl    20.47\n",
            "| epoch   3 |  1700/ 1938 batches | ms/batch 114.45 | loss  3.01 | ppl    20.25\n",
            "| epoch   3 |  1750/ 1938 batches | ms/batch 114.12 | loss  3.01 | ppl    20.23\n",
            "| epoch   3 |  1800/ 1938 batches | ms/batch 114.40 | loss  2.99 | ppl    19.98\n",
            "| epoch   3 |  1850/ 1938 batches | ms/batch 114.16 | loss  3.00 | ppl    20.08\n",
            "| epoch   3 |  1900/ 1938 batches | ms/batch 114.34 | loss  2.99 | ppl    19.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 311.56s | train loss  2.02 | train ppl     7.57 | valid loss  2.05 | valid ppl     7.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 3) to best_model_shakespeare_lstm.pt with Val PPL: 7.79 **\n",
            "| epoch   4 |    50/ 1938 batches | ms/batch 130.55 | loss  3.03 | ppl    20.62\n",
            "| epoch   4 |   100/ 1938 batches | ms/batch 114.61 | loss  2.98 | ppl    19.60\n",
            "| epoch   4 |   150/ 1938 batches | ms/batch 115.03 | loss  2.97 | ppl    19.40\n",
            "| epoch   4 |   200/ 1938 batches | ms/batch 114.29 | loss  2.97 | ppl    19.40\n",
            "| epoch   4 |   250/ 1938 batches | ms/batch 114.73 | loss  2.96 | ppl    19.38\n",
            "| epoch   4 |   300/ 1938 batches | ms/batch 114.81 | loss  2.96 | ppl    19.34\n",
            "| epoch   4 |   350/ 1938 batches | ms/batch 114.81 | loss  2.96 | ppl    19.23\n",
            "| epoch   4 |   400/ 1938 batches | ms/batch 114.43 | loss  2.95 | ppl    19.17\n",
            "| epoch   4 |   450/ 1938 batches | ms/batch 114.80 | loss  2.95 | ppl    19.13\n",
            "| epoch   4 |   500/ 1938 batches | ms/batch 114.79 | loss  2.96 | ppl    19.22\n",
            "| epoch   4 |   550/ 1938 batches | ms/batch 114.75 | loss  2.95 | ppl    19.08\n",
            "| epoch   4 |   600/ 1938 batches | ms/batch 114.44 | loss  2.95 | ppl    19.09\n",
            "| epoch   4 |   650/ 1938 batches | ms/batch 114.61 | loss  2.94 | ppl    18.90\n",
            "| epoch   4 |   700/ 1938 batches | ms/batch 114.52 | loss  2.94 | ppl    18.92\n",
            "| epoch   4 |   750/ 1938 batches | ms/batch 114.82 | loss  2.93 | ppl    18.73\n",
            "| epoch   4 |   800/ 1938 batches | ms/batch 114.83 | loss  2.93 | ppl    18.76\n",
            "| epoch   4 |   850/ 1938 batches | ms/batch 114.92 | loss  2.93 | ppl    18.81\n",
            "| epoch   4 |   900/ 1938 batches | ms/batch 114.72 | loss  2.92 | ppl    18.60\n",
            "| epoch   4 |   950/ 1938 batches | ms/batch 114.90 | loss  2.93 | ppl    18.64\n",
            "| epoch   4 |  1000/ 1938 batches | ms/batch 115.03 | loss  2.93 | ppl    18.71\n",
            "| epoch   4 |  1050/ 1938 batches | ms/batch 114.62 | loss  2.92 | ppl    18.54\n",
            "| epoch   4 |  1100/ 1938 batches | ms/batch 115.31 | loss  2.92 | ppl    18.54\n",
            "| epoch   4 |  1150/ 1938 batches | ms/batch 115.17 | loss  2.91 | ppl    18.40\n",
            "| epoch   4 |  1200/ 1938 batches | ms/batch 114.93 | loss  2.92 | ppl    18.46\n",
            "| epoch   4 |  1250/ 1938 batches | ms/batch 114.69 | loss  2.91 | ppl    18.28\n",
            "| epoch   4 |  1300/ 1938 batches | ms/batch 115.39 | loss  2.90 | ppl    18.15\n",
            "| epoch   4 |  1350/ 1938 batches | ms/batch 115.23 | loss  2.90 | ppl    18.25\n",
            "| epoch   4 |  1400/ 1938 batches | ms/batch 115.38 | loss  2.90 | ppl    18.16\n",
            "| epoch   4 |  1450/ 1938 batches | ms/batch 115.41 | loss  2.90 | ppl    18.09\n",
            "| epoch   4 |  1500/ 1938 batches | ms/batch 115.59 | loss  2.89 | ppl    18.04\n",
            "| epoch   4 |  1550/ 1938 batches | ms/batch 115.10 | loss  2.89 | ppl    18.03\n",
            "| epoch   4 |  1600/ 1938 batches | ms/batch 115.44 | loss  2.89 | ppl    17.99\n",
            "| epoch   4 |  1650/ 1938 batches | ms/batch 115.28 | loss  2.89 | ppl    17.95\n",
            "| epoch   4 |  1700/ 1938 batches | ms/batch 115.70 | loss  2.88 | ppl    17.80\n",
            "| epoch   4 |  1750/ 1938 batches | ms/batch 115.37 | loss  2.88 | ppl    17.87\n",
            "| epoch   4 |  1800/ 1938 batches | ms/batch 115.28 | loss  2.88 | ppl    17.76\n",
            "| epoch   4 |  1850/ 1938 batches | ms/batch 115.09 | loss  2.88 | ppl    17.81\n",
            "| epoch   4 |  1900/ 1938 batches | ms/batch 115.39 | loss  2.87 | ppl    17.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 312.06s | train loss  1.86 | train ppl     6.43 | valid loss  1.89 | valid ppl     6.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 4) to best_model_shakespeare_lstm.pt with Val PPL: 6.63 **\n",
            "| epoch   5 |    50/ 1938 batches | ms/batch 122.63 | loss  2.91 | ppl    18.31\n",
            "| epoch   5 |   100/ 1938 batches | ms/batch 114.81 | loss  2.86 | ppl    17.41\n",
            "| epoch   5 |   150/ 1938 batches | ms/batch 115.12 | loss  2.86 | ppl    17.40\n",
            "| epoch   5 |   200/ 1938 batches | ms/batch 115.03 | loss  2.85 | ppl    17.37\n",
            "| epoch   5 |   250/ 1938 batches | ms/batch 115.32 | loss  2.86 | ppl    17.39\n",
            "| epoch   5 |   300/ 1938 batches | ms/batch 115.02 | loss  2.84 | ppl    17.14\n",
            "| epoch   5 |   350/ 1938 batches | ms/batch 114.99 | loss  2.84 | ppl    17.16\n",
            "| epoch   5 |   400/ 1938 batches | ms/batch 114.81 | loss  2.84 | ppl    17.17\n",
            "| epoch   5 |   450/ 1938 batches | ms/batch 115.44 | loss  2.84 | ppl    17.08\n",
            "| epoch   5 |   500/ 1938 batches | ms/batch 115.34 | loss  2.84 | ppl    17.15\n",
            "| epoch   5 |   550/ 1938 batches | ms/batch 115.36 | loss  2.84 | ppl    17.06\n",
            "| epoch   5 |   600/ 1938 batches | ms/batch 115.46 | loss  2.84 | ppl    17.20\n",
            "| epoch   5 |   650/ 1938 batches | ms/batch 115.24 | loss  2.84 | ppl    17.07\n",
            "| epoch   5 |   700/ 1938 batches | ms/batch 115.14 | loss  2.83 | ppl    16.99\n",
            "| epoch   5 |   750/ 1938 batches | ms/batch 115.27 | loss  2.83 | ppl    17.02\n",
            "| epoch   5 |   800/ 1938 batches | ms/batch 115.13 | loss  2.83 | ppl    16.88\n",
            "| epoch   5 |   850/ 1938 batches | ms/batch 115.19 | loss  2.82 | ppl    16.79\n",
            "| epoch   5 |   900/ 1938 batches | ms/batch 115.10 | loss  2.82 | ppl    16.83\n",
            "| epoch   5 |   950/ 1938 batches | ms/batch 114.99 | loss  2.83 | ppl    16.94\n",
            "| epoch   5 |  1000/ 1938 batches | ms/batch 114.98 | loss  2.82 | ppl    16.80\n",
            "| epoch   5 |  1050/ 1938 batches | ms/batch 114.88 | loss  2.82 | ppl    16.70\n",
            "| epoch   5 |  1100/ 1938 batches | ms/batch 115.07 | loss  2.82 | ppl    16.84\n",
            "| epoch   5 |  1150/ 1938 batches | ms/batch 114.87 | loss  2.81 | ppl    16.62\n",
            "| epoch   5 |  1200/ 1938 batches | ms/batch 115.01 | loss  2.81 | ppl    16.61\n",
            "| epoch   5 |  1250/ 1938 batches | ms/batch 114.88 | loss  2.81 | ppl    16.65\n",
            "| epoch   5 |  1300/ 1938 batches | ms/batch 114.92 | loss  2.81 | ppl    16.57\n",
            "| epoch   5 |  1350/ 1938 batches | ms/batch 114.69 | loss  2.82 | ppl    16.72\n",
            "| epoch   5 |  1400/ 1938 batches | ms/batch 115.03 | loss  2.80 | ppl    16.49\n",
            "| epoch   5 |  1450/ 1938 batches | ms/batch 114.91 | loss  2.80 | ppl    16.48\n",
            "| epoch   5 |  1500/ 1938 batches | ms/batch 114.96 | loss  2.80 | ppl    16.52\n",
            "| epoch   5 |  1550/ 1938 batches | ms/batch 114.39 | loss  2.80 | ppl    16.47\n",
            "| epoch   5 |  1600/ 1938 batches | ms/batch 114.45 | loss  2.80 | ppl    16.40\n",
            "| epoch   5 |  1650/ 1938 batches | ms/batch 114.53 | loss  2.80 | ppl    16.38\n",
            "| epoch   5 |  1700/ 1938 batches | ms/batch 114.77 | loss  2.79 | ppl    16.27\n",
            "| epoch   5 |  1750/ 1938 batches | ms/batch 114.56 | loss  2.79 | ppl    16.28\n",
            "| epoch   5 |  1800/ 1938 batches | ms/batch 114.83 | loss  2.79 | ppl    16.21\n",
            "| epoch   5 |  1850/ 1938 batches | ms/batch 114.57 | loss  2.79 | ppl    16.26\n",
            "| epoch   5 |  1900/ 1938 batches | ms/batch 114.59 | loss  2.78 | ppl    16.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 311.22s | train loss  1.75 | train ppl     5.75 | valid loss  1.78 | valid ppl     5.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 5) to best_model_shakespeare_lstm.pt with Val PPL: 5.91 **\n",
            "| epoch   6 |    50/ 1938 batches | ms/batch 130.51 | loss  2.82 | ppl    16.79\n",
            "| epoch   6 |   100/ 1938 batches | ms/batch 114.54 | loss  2.76 | ppl    15.85\n",
            "| epoch   6 |   150/ 1938 batches | ms/batch 114.66 | loss  2.77 | ppl    15.99\n",
            "| epoch   6 |   200/ 1938 batches | ms/batch 114.39 | loss  2.77 | ppl    15.98\n",
            "| epoch   6 |   250/ 1938 batches | ms/batch 114.73 | loss  2.77 | ppl    15.98\n",
            "| epoch   6 |   300/ 1938 batches | ms/batch 114.62 | loss  2.77 | ppl    15.93\n",
            "| epoch   6 |   350/ 1938 batches | ms/batch 114.58 | loss  2.76 | ppl    15.80\n",
            "| epoch   6 |   400/ 1938 batches | ms/batch 114.40 | loss  2.76 | ppl    15.80\n",
            "| epoch   6 |   450/ 1938 batches | ms/batch 114.68 | loss  2.76 | ppl    15.74\n",
            "| epoch   6 |   500/ 1938 batches | ms/batch 114.56 | loss  2.76 | ppl    15.85\n",
            "| epoch   6 |   550/ 1938 batches | ms/batch 114.68 | loss  2.76 | ppl    15.73\n",
            "| epoch   6 |   600/ 1938 batches | ms/batch 114.36 | loss  2.76 | ppl    15.75\n",
            "| epoch   6 |   650/ 1938 batches | ms/batch 114.31 | loss  2.76 | ppl    15.80\n",
            "| epoch   6 |   700/ 1938 batches | ms/batch 114.32 | loss  2.76 | ppl    15.81\n",
            "| epoch   6 |   750/ 1938 batches | ms/batch 114.37 | loss  2.75 | ppl    15.66\n",
            "| epoch   6 |   800/ 1938 batches | ms/batch 114.43 | loss  2.75 | ppl    15.62\n",
            "| epoch   6 |   850/ 1938 batches | ms/batch 114.20 | loss  2.75 | ppl    15.59\n",
            "| epoch   6 |   900/ 1938 batches | ms/batch 114.59 | loss  2.74 | ppl    15.51\n",
            "| epoch   6 |   950/ 1938 batches | ms/batch 114.51 | loss  2.75 | ppl    15.59\n",
            "| epoch   6 |  1000/ 1938 batches | ms/batch 114.68 | loss  2.75 | ppl    15.60\n",
            "| epoch   6 |  1050/ 1938 batches | ms/batch 114.37 | loss  2.73 | ppl    15.40\n",
            "| epoch   6 |  1100/ 1938 batches | ms/batch 114.69 | loss  2.75 | ppl    15.57\n",
            "| epoch   6 |  1150/ 1938 batches | ms/batch 114.31 | loss  2.75 | ppl    15.58\n",
            "| epoch   6 |  1200/ 1938 batches | ms/batch 114.67 | loss  2.74 | ppl    15.44\n",
            "| epoch   6 |  1250/ 1938 batches | ms/batch 114.15 | loss  2.74 | ppl    15.46\n",
            "| epoch   6 |  1300/ 1938 batches | ms/batch 114.40 | loss  2.73 | ppl    15.35\n",
            "| epoch   6 |  1350/ 1938 batches | ms/batch 113.89 | loss  2.73 | ppl    15.33\n",
            "| epoch   6 |  1400/ 1938 batches | ms/batch 114.52 | loss  2.73 | ppl    15.34\n",
            "| epoch   6 |  1450/ 1938 batches | ms/batch 114.29 | loss  2.72 | ppl    15.21\n",
            "| epoch   6 |  1500/ 1938 batches | ms/batch 114.87 | loss  2.73 | ppl    15.29\n",
            "| epoch   6 |  1550/ 1938 batches | ms/batch 114.87 | loss  2.73 | ppl    15.29\n",
            "| epoch   6 |  1600/ 1938 batches | ms/batch 114.83 | loss  2.73 | ppl    15.34\n",
            "| epoch   6 |  1650/ 1938 batches | ms/batch 114.69 | loss  2.72 | ppl    15.25\n",
            "| epoch   6 |  1700/ 1938 batches | ms/batch 115.02 | loss  2.72 | ppl    15.19\n",
            "| epoch   6 |  1750/ 1938 batches | ms/batch 115.04 | loss  2.72 | ppl    15.20\n",
            "| epoch   6 |  1800/ 1938 batches | ms/batch 115.20 | loss  2.72 | ppl    15.22\n",
            "| epoch   6 |  1850/ 1938 batches | ms/batch 115.25 | loss  2.71 | ppl    15.06\n",
            "| epoch   6 |  1900/ 1938 batches | ms/batch 115.12 | loss  2.72 | ppl    15.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 311.17s | train loss  1.66 | train ppl     5.26 | valid loss  1.69 | valid ppl     5.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 6) to best_model_shakespeare_lstm.pt with Val PPL: 5.41 **\n",
            "| epoch   7 |    50/ 1938 batches | ms/batch 130.22 | loss  2.75 | ppl    15.68\n",
            "| epoch   7 |   100/ 1938 batches | ms/batch 114.96 | loss  2.70 | ppl    14.87\n",
            "| epoch   7 |   150/ 1938 batches | ms/batch 115.07 | loss  2.70 | ppl    14.91\n",
            "| epoch   7 |   200/ 1938 batches | ms/batch 114.78 | loss  2.69 | ppl    14.73\n",
            "| epoch   7 |   250/ 1938 batches | ms/batch 115.30 | loss  2.70 | ppl    14.87\n",
            "| epoch   7 |   300/ 1938 batches | ms/batch 115.18 | loss  2.69 | ppl    14.75\n",
            "| epoch   7 |   350/ 1938 batches | ms/batch 115.43 | loss  2.70 | ppl    14.83\n",
            "| epoch   7 |   400/ 1938 batches | ms/batch 115.34 | loss  2.70 | ppl    14.81\n",
            "| epoch   7 |   450/ 1938 batches | ms/batch 115.40 | loss  2.69 | ppl    14.80\n",
            "| epoch   7 |   500/ 1938 batches | ms/batch 115.22 | loss  2.69 | ppl    14.71\n",
            "| epoch   7 |   550/ 1938 batches | ms/batch 115.30 | loss  2.69 | ppl    14.79\n",
            "| epoch   7 |   600/ 1938 batches | ms/batch 115.19 | loss  2.69 | ppl    14.75\n",
            "| epoch   7 |   650/ 1938 batches | ms/batch 115.37 | loss  2.69 | ppl    14.69\n",
            "| epoch   7 |   700/ 1938 batches | ms/batch 115.34 | loss  2.69 | ppl    14.76\n",
            "| epoch   7 |   750/ 1938 batches | ms/batch 115.35 | loss  2.69 | ppl    14.67\n",
            "| epoch   7 |   800/ 1938 batches | ms/batch 115.19 | loss  2.69 | ppl    14.79\n",
            "| epoch   7 |   850/ 1938 batches | ms/batch 114.99 | loss  2.69 | ppl    14.67\n",
            "| epoch   7 |   900/ 1938 batches | ms/batch 115.08 | loss  2.69 | ppl    14.76\n",
            "| epoch   7 |   950/ 1938 batches | ms/batch 115.00 | loss  2.68 | ppl    14.63\n",
            "| epoch   7 |  1000/ 1938 batches | ms/batch 115.29 | loss  2.68 | ppl    14.60\n",
            "| epoch   7 |  1050/ 1938 batches | ms/batch 114.92 | loss  2.67 | ppl    14.47\n",
            "| epoch   7 |  1100/ 1938 batches | ms/batch 115.08 | loss  2.69 | ppl    14.68\n",
            "| epoch   7 |  1150/ 1938 batches | ms/batch 114.90 | loss  2.68 | ppl    14.54\n",
            "| epoch   7 |  1200/ 1938 batches | ms/batch 115.01 | loss  2.67 | ppl    14.47\n",
            "| epoch   7 |  1250/ 1938 batches | ms/batch 115.09 | loss  2.67 | ppl    14.43\n",
            "| epoch   7 |  1300/ 1938 batches | ms/batch 114.86 | loss  2.68 | ppl    14.54\n",
            "| epoch   7 |  1350/ 1938 batches | ms/batch 114.56 | loss  2.67 | ppl    14.50\n",
            "| epoch   7 |  1400/ 1938 batches | ms/batch 114.58 | loss  2.68 | ppl    14.58\n",
            "| epoch   7 |  1450/ 1938 batches | ms/batch 114.68 | loss  2.68 | ppl    14.52\n",
            "| epoch   7 |  1500/ 1938 batches | ms/batch 115.08 | loss  2.67 | ppl    14.41\n",
            "| epoch   7 |  1550/ 1938 batches | ms/batch 114.54 | loss  2.67 | ppl    14.42\n",
            "| epoch   7 |  1600/ 1938 batches | ms/batch 114.92 | loss  2.65 | ppl    14.20\n",
            "| epoch   7 |  1650/ 1938 batches | ms/batch 114.83 | loss  2.66 | ppl    14.37\n",
            "| epoch   7 |  1700/ 1938 batches | ms/batch 114.79 | loss  2.67 | ppl    14.44\n",
            "| epoch   7 |  1750/ 1938 batches | ms/batch 114.62 | loss  2.66 | ppl    14.28\n",
            "| epoch   7 |  1800/ 1938 batches | ms/batch 114.70 | loss  2.66 | ppl    14.30\n",
            "| epoch   7 |  1850/ 1938 batches | ms/batch 114.86 | loss  2.66 | ppl    14.28\n",
            "| epoch   7 |  1900/ 1938 batches | ms/batch 114.64 | loss  2.65 | ppl    14.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 311.90s | train loss  1.59 | train ppl     4.90 | valid loss  1.62 | valid ppl     5.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 7) to best_model_shakespeare_lstm.pt with Val PPL: 5.05 **\n",
            "| epoch   8 |    50/ 1938 batches | ms/batch 130.90 | loss  2.69 | ppl    14.70\n",
            "| epoch   8 |   100/ 1938 batches | ms/batch 115.80 | loss  2.65 | ppl    14.13\n",
            "| epoch   8 |   150/ 1938 batches | ms/batch 115.76 | loss  2.64 | ppl    14.01\n",
            "| epoch   8 |   200/ 1938 batches | ms/batch 115.33 | loss  2.65 | ppl    14.08\n",
            "| epoch   8 |   250/ 1938 batches | ms/batch 115.77 | loss  2.64 | ppl    14.00\n",
            "| epoch   8 |   300/ 1938 batches | ms/batch 115.28 | loss  2.64 | ppl    14.06\n",
            "| epoch   8 |   350/ 1938 batches | ms/batch 115.33 | loss  2.65 | ppl    14.13\n",
            "| epoch   8 |   400/ 1938 batches | ms/batch 114.98 | loss  2.64 | ppl    13.99\n",
            "| epoch   8 |   450/ 1938 batches | ms/batch 114.71 | loss  2.64 | ppl    13.99\n",
            "| epoch   8 |   500/ 1938 batches | ms/batch 114.60 | loss  2.63 | ppl    13.94\n",
            "| epoch   8 |   550/ 1938 batches | ms/batch 114.64 | loss  2.63 | ppl    13.82\n",
            "| epoch   8 |   600/ 1938 batches | ms/batch 114.87 | loss  2.64 | ppl    13.97\n",
            "| epoch   8 |   650/ 1938 batches | ms/batch 114.50 | loss  2.64 | ppl    13.97\n",
            "| epoch   8 |   700/ 1938 batches | ms/batch 114.47 | loss  2.64 | ppl    13.94\n",
            "| epoch   8 |   750/ 1938 batches | ms/batch 114.41 | loss  2.64 | ppl    13.95\n",
            "| epoch   8 |   800/ 1938 batches | ms/batch 114.50 | loss  2.64 | ppl    14.02\n",
            "| epoch   8 |   850/ 1938 batches | ms/batch 114.09 | loss  2.63 | ppl    13.86\n",
            "| epoch   8 |   900/ 1938 batches | ms/batch 114.94 | loss  2.63 | ppl    13.87\n",
            "| epoch   8 |   950/ 1938 batches | ms/batch 114.38 | loss  2.63 | ppl    13.86\n",
            "| epoch   8 |  1000/ 1938 batches | ms/batch 114.53 | loss  2.63 | ppl    13.84\n",
            "| epoch   8 |  1050/ 1938 batches | ms/batch 114.06 | loss  2.63 | ppl    13.88\n",
            "| epoch   8 |  1100/ 1938 batches | ms/batch 114.39 | loss  2.63 | ppl    13.87\n",
            "| epoch   8 |  1150/ 1938 batches | ms/batch 114.21 | loss  2.62 | ppl    13.79\n",
            "| epoch   8 |  1200/ 1938 batches | ms/batch 114.58 | loss  2.62 | ppl    13.80\n",
            "| epoch   8 |  1250/ 1938 batches | ms/batch 114.56 | loss  2.63 | ppl    13.84\n",
            "| epoch   8 |  1300/ 1938 batches | ms/batch 114.47 | loss  2.62 | ppl    13.74\n",
            "| epoch   8 |  1350/ 1938 batches | ms/batch 114.41 | loss  2.62 | ppl    13.75\n",
            "| epoch   8 |  1400/ 1938 batches | ms/batch 114.34 | loss  2.62 | ppl    13.77\n",
            "| epoch   8 |  1450/ 1938 batches | ms/batch 114.34 | loss  2.62 | ppl    13.72\n",
            "| epoch   8 |  1500/ 1938 batches | ms/batch 114.44 | loss  2.62 | ppl    13.74\n",
            "| epoch   8 |  1550/ 1938 batches | ms/batch 114.34 | loss  2.62 | ppl    13.68\n",
            "| epoch   8 |  1600/ 1938 batches | ms/batch 114.65 | loss  2.62 | ppl    13.71\n",
            "| epoch   8 |  1650/ 1938 batches | ms/batch 114.38 | loss  2.61 | ppl    13.59\n",
            "| epoch   8 |  1700/ 1938 batches | ms/batch 113.96 | loss  2.61 | ppl    13.67\n",
            "| epoch   8 |  1750/ 1938 batches | ms/batch 114.31 | loss  2.61 | ppl    13.66\n",
            "| epoch   8 |  1800/ 1938 batches | ms/batch 114.04 | loss  2.61 | ppl    13.62\n",
            "| epoch   8 |  1850/ 1938 batches | ms/batch 114.39 | loss  2.61 | ppl    13.61\n",
            "| epoch   8 |  1900/ 1938 batches | ms/batch 114.25 | loss  2.61 | ppl    13.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 311.27s | train loss  1.53 | train ppl     4.64 | valid loss  1.56 | valid ppl     4.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 8) to best_model_shakespeare_lstm.pt with Val PPL: 4.77 **\n",
            "| epoch   9 |    50/ 1938 batches | ms/batch 129.88 | loss  2.64 | ppl    14.04\n",
            "| epoch   9 |   100/ 1938 batches | ms/batch 114.17 | loss  2.59 | ppl    13.37\n",
            "| epoch   9 |   150/ 1938 batches | ms/batch 114.42 | loss  2.60 | ppl    13.41\n",
            "| epoch   9 |   200/ 1938 batches | ms/batch 114.33 | loss  2.59 | ppl    13.36\n",
            "| epoch   9 |   250/ 1938 batches | ms/batch 114.21 | loss  2.60 | ppl    13.48\n",
            "| epoch   9 |   300/ 1938 batches | ms/batch 114.16 | loss  2.59 | ppl    13.30\n",
            "| epoch   9 |   350/ 1938 batches | ms/batch 114.29 | loss  2.59 | ppl    13.36\n",
            "| epoch   9 |   400/ 1938 batches | ms/batch 114.62 | loss  2.59 | ppl    13.32\n",
            "| epoch   9 |   450/ 1938 batches | ms/batch 114.72 | loss  2.58 | ppl    13.25\n",
            "| epoch   9 |   500/ 1938 batches | ms/batch 114.61 | loss  2.59 | ppl    13.35\n",
            "| epoch   9 |   550/ 1938 batches | ms/batch 114.39 | loss  2.59 | ppl    13.32\n",
            "| epoch   9 |   600/ 1938 batches | ms/batch 114.55 | loss  2.59 | ppl    13.35\n",
            "| epoch   9 |   650/ 1938 batches | ms/batch 114.23 | loss  2.59 | ppl    13.29\n",
            "| epoch   9 |   700/ 1938 batches | ms/batch 114.57 | loss  2.58 | ppl    13.24\n",
            "| epoch   9 |   750/ 1938 batches | ms/batch 114.88 | loss  2.59 | ppl    13.27\n",
            "| epoch   9 |   800/ 1938 batches | ms/batch 114.97 | loss  2.58 | ppl    13.16\n",
            "| epoch   9 |   850/ 1938 batches | ms/batch 114.67 | loss  2.58 | ppl    13.22\n",
            "| epoch   9 |   900/ 1938 batches | ms/batch 114.96 | loss  2.58 | ppl    13.22\n",
            "| epoch   9 |   950/ 1938 batches | ms/batch 114.99 | loss  2.58 | ppl    13.14\n",
            "| epoch   9 |  1000/ 1938 batches | ms/batch 115.48 | loss  2.58 | ppl    13.24\n",
            "| epoch   9 |  1050/ 1938 batches | ms/batch 115.04 | loss  2.58 | ppl    13.22\n",
            "| epoch   9 |  1100/ 1938 batches | ms/batch 115.39 | loss  2.59 | ppl    13.28\n",
            "| epoch   9 |  1150/ 1938 batches | ms/batch 115.03 | loss  2.58 | ppl    13.17\n",
            "| epoch   9 |  1200/ 1938 batches | ms/batch 115.28 | loss  2.58 | ppl    13.14\n",
            "| epoch   9 |  1250/ 1938 batches | ms/batch 114.79 | loss  2.59 | ppl    13.26\n",
            "| epoch   9 |  1300/ 1938 batches | ms/batch 114.99 | loss  2.57 | ppl    13.12\n",
            "| epoch   9 |  1350/ 1938 batches | ms/batch 114.98 | loss  2.57 | ppl    13.09\n",
            "| epoch   9 |  1400/ 1938 batches | ms/batch 114.95 | loss  2.58 | ppl    13.14\n",
            "| epoch   9 |  1450/ 1938 batches | ms/batch 114.79 | loss  2.58 | ppl    13.15\n",
            "| epoch   9 |  1500/ 1938 batches | ms/batch 114.70 | loss  2.57 | ppl    13.12\n",
            "| epoch   9 |  1550/ 1938 batches | ms/batch 114.94 | loss  2.57 | ppl    13.07\n",
            "| epoch   9 |  1600/ 1938 batches | ms/batch 114.64 | loss  2.57 | ppl    13.09\n",
            "| epoch   9 |  1650/ 1938 batches | ms/batch 114.93 | loss  2.57 | ppl    13.09\n",
            "| epoch   9 |  1700/ 1938 batches | ms/batch 114.60 | loss  2.57 | ppl    13.02\n",
            "| epoch   9 |  1750/ 1938 batches | ms/batch 115.06 | loss  2.57 | ppl    13.03\n",
            "| epoch   9 |  1800/ 1938 batches | ms/batch 114.34 | loss  2.56 | ppl    12.93\n",
            "| epoch   9 |  1850/ 1938 batches | ms/batch 114.65 | loss  2.57 | ppl    13.09\n",
            "| epoch   9 |  1900/ 1938 batches | ms/batch 114.41 | loss  2.57 | ppl    13.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 311.16s | train loss  1.48 | train ppl     4.40 | valid loss  1.51 | valid ppl     4.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 9) to best_model_shakespeare_lstm.pt with Val PPL: 4.52 **\n",
            "| epoch  10 |    50/ 1938 batches | ms/batch 129.62 | loss  2.60 | ppl    13.49\n",
            "| epoch  10 |   100/ 1938 batches | ms/batch 114.96 | loss  2.55 | ppl    12.84\n",
            "| epoch  10 |   150/ 1938 batches | ms/batch 114.59 | loss  2.55 | ppl    12.78\n",
            "| epoch  10 |   200/ 1938 batches | ms/batch 115.19 | loss  2.55 | ppl    12.82\n",
            "| epoch  10 |   250/ 1938 batches | ms/batch 114.72 | loss  2.55 | ppl    12.83\n",
            "| epoch  10 |   300/ 1938 batches | ms/batch 114.93 | loss  2.55 | ppl    12.83\n",
            "| epoch  10 |   350/ 1938 batches | ms/batch 114.84 | loss  2.55 | ppl    12.77\n",
            "| epoch  10 |   400/ 1938 batches | ms/batch 115.48 | loss  2.54 | ppl    12.62\n",
            "| epoch  10 |   450/ 1938 batches | ms/batch 115.27 | loss  2.55 | ppl    12.83\n",
            "| epoch  10 |   500/ 1938 batches | ms/batch 115.63 | loss  2.55 | ppl    12.76\n",
            "| epoch  10 |   550/ 1938 batches | ms/batch 115.35 | loss  2.55 | ppl    12.81\n",
            "| epoch  10 |   600/ 1938 batches | ms/batch 115.37 | loss  2.55 | ppl    12.79\n",
            "| epoch  10 |   650/ 1938 batches | ms/batch 115.30 | loss  2.54 | ppl    12.73\n",
            "| epoch  10 |   700/ 1938 batches | ms/batch 115.70 | loss  2.54 | ppl    12.64\n",
            "| epoch  10 |   750/ 1938 batches | ms/batch 115.06 | loss  2.55 | ppl    12.82\n",
            "| epoch  10 |   800/ 1938 batches | ms/batch 115.37 | loss  2.55 | ppl    12.84\n",
            "| epoch  10 |   850/ 1938 batches | ms/batch 114.95 | loss  2.54 | ppl    12.70\n",
            "| epoch  10 |   900/ 1938 batches | ms/batch 115.09 | loss  2.54 | ppl    12.70\n",
            "| epoch  10 |   950/ 1938 batches | ms/batch 114.90 | loss  2.55 | ppl    12.76\n",
            "| epoch  10 |  1000/ 1938 batches | ms/batch 114.94 | loss  2.55 | ppl    12.75\n",
            "| epoch  10 |  1050/ 1938 batches | ms/batch 114.79 | loss  2.54 | ppl    12.64\n",
            "| epoch  10 |  1100/ 1938 batches | ms/batch 114.90 | loss  2.53 | ppl    12.60\n",
            "| epoch  10 |  1150/ 1938 batches | ms/batch 114.95 | loss  2.54 | ppl    12.73\n",
            "| epoch  10 |  1200/ 1938 batches | ms/batch 114.73 | loss  2.54 | ppl    12.62\n",
            "| epoch  10 |  1250/ 1938 batches | ms/batch 115.09 | loss  2.53 | ppl    12.54\n",
            "| epoch  10 |  1300/ 1938 batches | ms/batch 114.63 | loss  2.53 | ppl    12.57\n",
            "| epoch  10 |  1350/ 1938 batches | ms/batch 114.88 | loss  2.53 | ppl    12.56\n",
            "| epoch  10 |  1400/ 1938 batches | ms/batch 114.65 | loss  2.53 | ppl    12.61\n",
            "| epoch  10 |  1450/ 1938 batches | ms/batch 114.95 | loss  2.53 | ppl    12.61\n",
            "| epoch  10 |  1500/ 1938 batches | ms/batch 114.61 | loss  2.54 | ppl    12.63\n",
            "| epoch  10 |  1550/ 1938 batches | ms/batch 114.75 | loss  2.53 | ppl    12.54\n",
            "| epoch  10 |  1600/ 1938 batches | ms/batch 114.62 | loss  2.52 | ppl    12.38\n",
            "| epoch  10 |  1650/ 1938 batches | ms/batch 115.00 | loss  2.53 | ppl    12.61\n",
            "| epoch  10 |  1700/ 1938 batches | ms/batch 114.73 | loss  2.53 | ppl    12.55\n",
            "| epoch  10 |  1750/ 1938 batches | ms/batch 115.11 | loss  2.53 | ppl    12.51\n",
            "| epoch  10 |  1800/ 1938 batches | ms/batch 114.60 | loss  2.52 | ppl    12.48\n",
            "| epoch  10 |  1850/ 1938 batches | ms/batch 114.79 | loss  2.53 | ppl    12.57\n",
            "| epoch  10 |  1900/ 1938 batches | ms/batch 114.68 | loss  2.52 | ppl    12.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 311.68s | train loss  1.43 | train ppl     4.20 | valid loss  1.46 | valid ppl     4.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 10) to best_model_shakespeare_lstm.pt with Val PPL: 4.32 **\n",
            "| epoch  11 |    50/ 1938 batches | ms/batch 129.18 | loss  2.56 | ppl    12.88\n",
            "| epoch  11 |   100/ 1938 batches | ms/batch 114.23 | loss  2.51 | ppl    12.34\n",
            "| epoch  11 |   150/ 1938 batches | ms/batch 114.58 | loss  2.52 | ppl    12.39\n",
            "| epoch  11 |   200/ 1938 batches | ms/batch 115.20 | loss  2.51 | ppl    12.32\n",
            "| epoch  11 |   250/ 1938 batches | ms/batch 114.98 | loss  2.51 | ppl    12.26\n",
            "| epoch  11 |   300/ 1938 batches | ms/batch 115.35 | loss  2.52 | ppl    12.37\n",
            "| epoch  11 |   350/ 1938 batches | ms/batch 115.25 | loss  2.52 | ppl    12.37\n",
            "| epoch  11 |   400/ 1938 batches | ms/batch 115.67 | loss  2.51 | ppl    12.34\n",
            "| epoch  11 |   450/ 1938 batches | ms/batch 115.48 | loss  2.52 | ppl    12.41\n",
            "| epoch  11 |   500/ 1938 batches | ms/batch 115.68 | loss  2.51 | ppl    12.33\n",
            "| epoch  11 |   550/ 1938 batches | ms/batch 115.30 | loss  2.51 | ppl    12.28\n",
            "| epoch  11 |   600/ 1938 batches | ms/batch 115.10 | loss  2.51 | ppl    12.31\n",
            "| epoch  11 |   650/ 1938 batches | ms/batch 114.98 | loss  2.50 | ppl    12.24\n",
            "| epoch  11 |   700/ 1938 batches | ms/batch 114.94 | loss  2.51 | ppl    12.33\n",
            "| epoch  11 |   750/ 1938 batches | ms/batch 114.75 | loss  2.49 | ppl    12.12\n",
            "| epoch  11 |   800/ 1938 batches | ms/batch 114.86 | loss  2.51 | ppl    12.28\n",
            "| epoch  11 |   850/ 1938 batches | ms/batch 114.67 | loss  2.51 | ppl    12.27\n",
            "| epoch  11 |   900/ 1938 batches | ms/batch 114.88 | loss  2.51 | ppl    12.25\n",
            "| epoch  11 |   950/ 1938 batches | ms/batch 114.96 | loss  2.50 | ppl    12.16\n",
            "| epoch  11 |  1000/ 1938 batches | ms/batch 114.60 | loss  2.50 | ppl    12.21\n",
            "| epoch  11 |  1050/ 1938 batches | ms/batch 114.85 | loss  2.50 | ppl    12.19\n",
            "| epoch  11 |  1100/ 1938 batches | ms/batch 114.76 | loss  2.50 | ppl    12.17\n",
            "| epoch  11 |  1150/ 1938 batches | ms/batch 114.91 | loss  2.51 | ppl    12.28\n",
            "| epoch  11 |  1200/ 1938 batches | ms/batch 114.37 | loss  2.50 | ppl    12.20\n",
            "| epoch  11 |  1250/ 1938 batches | ms/batch 114.84 | loss  2.49 | ppl    12.06\n",
            "| epoch  11 |  1300/ 1938 batches | ms/batch 114.23 | loss  2.50 | ppl    12.16\n",
            "| epoch  11 |  1350/ 1938 batches | ms/batch 114.62 | loss  2.49 | ppl    12.12\n",
            "| epoch  11 |  1400/ 1938 batches | ms/batch 114.48 | loss  2.49 | ppl    12.09\n",
            "| epoch  11 |  1450/ 1938 batches | ms/batch 114.81 | loss  2.49 | ppl    12.07\n",
            "| epoch  11 |  1500/ 1938 batches | ms/batch 114.35 | loss  2.49 | ppl    12.07\n",
            "| epoch  11 |  1550/ 1938 batches | ms/batch 114.58 | loss  2.49 | ppl    12.09\n",
            "| epoch  11 |  1600/ 1938 batches | ms/batch 114.10 | loss  2.49 | ppl    12.06\n",
            "| epoch  11 |  1650/ 1938 batches | ms/batch 114.35 | loss  2.49 | ppl    12.12\n",
            "| epoch  11 |  1700/ 1938 batches | ms/batch 114.57 | loss  2.49 | ppl    12.07\n",
            "| epoch  11 |  1750/ 1938 batches | ms/batch 114.66 | loss  2.49 | ppl    12.12\n",
            "| epoch  11 |  1800/ 1938 batches | ms/batch 114.31 | loss  2.49 | ppl    12.07\n",
            "| epoch  11 |  1850/ 1938 batches | ms/batch 114.49 | loss  2.48 | ppl    11.98\n",
            "| epoch  11 |  1900/ 1938 batches | ms/batch 114.19 | loss  2.49 | ppl    12.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 311.60s | train loss  1.39 | train ppl     4.01 | valid loss  1.42 | valid ppl     4.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 11) to best_model_shakespeare_lstm.pt with Val PPL: 4.13 **\n",
            "| epoch  12 |    50/ 1938 batches | ms/batch 122.29 | loss  2.52 | ppl    12.39\n",
            "| epoch  12 |   100/ 1938 batches | ms/batch 114.99 | loss  2.47 | ppl    11.86\n",
            "| epoch  12 |   150/ 1938 batches | ms/batch 114.84 | loss  2.47 | ppl    11.83\n",
            "| epoch  12 |   200/ 1938 batches | ms/batch 115.03 | loss  2.48 | ppl    11.91\n",
            "| epoch  12 |   250/ 1938 batches | ms/batch 114.47 | loss  2.48 | ppl    11.91\n",
            "| epoch  12 |   300/ 1938 batches | ms/batch 114.82 | loss  2.48 | ppl    11.91\n",
            "| epoch  12 |   350/ 1938 batches | ms/batch 114.53 | loss  2.48 | ppl    11.89\n",
            "| epoch  12 |   400/ 1938 batches | ms/batch 114.57 | loss  2.47 | ppl    11.86\n",
            "| epoch  12 |   450/ 1938 batches | ms/batch 114.37 | loss  2.47 | ppl    11.78\n",
            "| epoch  12 |   500/ 1938 batches | ms/batch 114.65 | loss  2.48 | ppl    11.92\n",
            "| epoch  12 |   550/ 1938 batches | ms/batch 114.63 | loss  2.48 | ppl    11.91\n",
            "| epoch  12 |   600/ 1938 batches | ms/batch 114.57 | loss  2.48 | ppl    11.92\n",
            "| epoch  12 |   650/ 1938 batches | ms/batch 114.47 | loss  2.48 | ppl    11.90\n",
            "| epoch  12 |   700/ 1938 batches | ms/batch 114.42 | loss  2.48 | ppl    11.91\n",
            "| epoch  12 |   750/ 1938 batches | ms/batch 114.68 | loss  2.48 | ppl    11.94\n",
            "| epoch  12 |   800/ 1938 batches | ms/batch 114.39 | loss  2.47 | ppl    11.81\n",
            "| epoch  12 |   850/ 1938 batches | ms/batch 114.60 | loss  2.47 | ppl    11.82\n",
            "| epoch  12 |   900/ 1938 batches | ms/batch 114.18 | loss  2.47 | ppl    11.80\n",
            "| epoch  12 |   950/ 1938 batches | ms/batch 114.42 | loss  2.47 | ppl    11.81\n",
            "| epoch  12 |  1000/ 1938 batches | ms/batch 114.26 | loss  2.47 | ppl    11.81\n",
            "| epoch  12 |  1050/ 1938 batches | ms/batch 114.42 | loss  2.48 | ppl    11.88\n",
            "| epoch  12 |  1100/ 1938 batches | ms/batch 114.40 | loss  2.47 | ppl    11.85\n",
            "| epoch  12 |  1150/ 1938 batches | ms/batch 114.78 | loss  2.47 | ppl    11.79\n",
            "| epoch  12 |  1200/ 1938 batches | ms/batch 114.48 | loss  2.47 | ppl    11.79\n",
            "| epoch  12 |  1250/ 1938 batches | ms/batch 114.98 | loss  2.46 | ppl    11.76\n",
            "| epoch  12 |  1300/ 1938 batches | ms/batch 114.68 | loss  2.46 | ppl    11.73\n",
            "| epoch  12 |  1350/ 1938 batches | ms/batch 114.96 | loss  2.46 | ppl    11.69\n",
            "| epoch  12 |  1400/ 1938 batches | ms/batch 114.84 | loss  2.47 | ppl    11.78\n",
            "| epoch  12 |  1450/ 1938 batches | ms/batch 115.29 | loss  2.47 | ppl    11.80\n",
            "| epoch  12 |  1500/ 1938 batches | ms/batch 114.99 | loss  2.46 | ppl    11.65\n",
            "| epoch  12 |  1550/ 1938 batches | ms/batch 114.87 | loss  2.46 | ppl    11.73\n",
            "| epoch  12 |  1600/ 1938 batches | ms/batch 114.66 | loss  2.46 | ppl    11.71\n",
            "| epoch  12 |  1650/ 1938 batches | ms/batch 115.05 | loss  2.46 | ppl    11.69\n",
            "| epoch  12 |  1700/ 1938 batches | ms/batch 114.93 | loss  2.46 | ppl    11.69\n",
            "| epoch  12 |  1750/ 1938 batches | ms/batch 115.03 | loss  2.46 | ppl    11.69\n",
            "| epoch  12 |  1800/ 1938 batches | ms/batch 115.24 | loss  2.46 | ppl    11.73\n",
            "| epoch  12 |  1850/ 1938 batches | ms/batch 114.92 | loss  2.45 | ppl    11.63\n",
            "| epoch  12 |  1900/ 1938 batches | ms/batch 115.22 | loss  2.46 | ppl    11.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 310.87s | train loss  1.35 | train ppl     3.87 | valid loss  1.38 | valid ppl     3.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 12) to best_model_shakespeare_lstm.pt with Val PPL: 3.99 **\n",
            "| epoch  13 |    50/ 1938 batches | ms/batch 132.46 | loss  2.49 | ppl    12.04\n",
            "| epoch  13 |   100/ 1938 batches | ms/batch 114.79 | loss  2.44 | ppl    11.48\n",
            "| epoch  13 |   150/ 1938 batches | ms/batch 114.84 | loss  2.43 | ppl    11.40\n",
            "| epoch  13 |   200/ 1938 batches | ms/batch 114.85 | loss  2.44 | ppl    11.48\n",
            "| epoch  13 |   250/ 1938 batches | ms/batch 115.32 | loss  2.44 | ppl    11.51\n",
            "| epoch  13 |   300/ 1938 batches | ms/batch 115.23 | loss  2.45 | ppl    11.55\n",
            "| epoch  13 |   350/ 1938 batches | ms/batch 115.09 | loss  2.45 | ppl    11.55\n",
            "| epoch  13 |   400/ 1938 batches | ms/batch 115.09 | loss  2.44 | ppl    11.45\n",
            "| epoch  13 |   450/ 1938 batches | ms/batch 115.24 | loss  2.45 | ppl    11.56\n",
            "| epoch  13 |   500/ 1938 batches | ms/batch 115.39 | loss  2.44 | ppl    11.47\n",
            "| epoch  13 |   550/ 1938 batches | ms/batch 115.49 | loss  2.45 | ppl    11.55\n",
            "| epoch  13 |   600/ 1938 batches | ms/batch 115.30 | loss  2.44 | ppl    11.44\n",
            "| epoch  13 |   650/ 1938 batches | ms/batch 115.27 | loss  2.45 | ppl    11.55\n",
            "| epoch  13 |   700/ 1938 batches | ms/batch 114.92 | loss  2.45 | ppl    11.56\n",
            "| epoch  13 |   750/ 1938 batches | ms/batch 114.76 | loss  2.44 | ppl    11.48\n",
            "| epoch  13 |   800/ 1938 batches | ms/batch 114.54 | loss  2.43 | ppl    11.41\n",
            "| epoch  13 |   850/ 1938 batches | ms/batch 114.86 | loss  2.44 | ppl    11.50\n",
            "| epoch  13 |   900/ 1938 batches | ms/batch 114.82 | loss  2.44 | ppl    11.47\n",
            "| epoch  13 |   950/ 1938 batches | ms/batch 114.60 | loss  2.43 | ppl    11.41\n",
            "| epoch  13 |  1000/ 1938 batches | ms/batch 114.19 | loss  2.44 | ppl    11.47\n",
            "| epoch  13 |  1050/ 1938 batches | ms/batch 114.43 | loss  2.44 | ppl    11.48\n",
            "| epoch  13 |  1100/ 1938 batches | ms/batch 114.18 | loss  2.44 | ppl    11.50\n",
            "| epoch  13 |  1150/ 1938 batches | ms/batch 114.78 | loss  2.44 | ppl    11.47\n",
            "| epoch  13 |  1200/ 1938 batches | ms/batch 114.62 | loss  2.44 | ppl    11.42\n",
            "| epoch  13 |  1250/ 1938 batches | ms/batch 114.48 | loss  2.44 | ppl    11.43\n",
            "| epoch  13 |  1300/ 1938 batches | ms/batch 114.53 | loss  2.43 | ppl    11.40\n",
            "| epoch  13 |  1350/ 1938 batches | ms/batch 114.67 | loss  2.44 | ppl    11.42\n",
            "| epoch  13 |  1400/ 1938 batches | ms/batch 114.36 | loss  2.44 | ppl    11.48\n",
            "| epoch  13 |  1450/ 1938 batches | ms/batch 114.50 | loss  2.43 | ppl    11.36\n",
            "| epoch  13 |  1500/ 1938 batches | ms/batch 114.62 | loss  2.43 | ppl    11.41\n",
            "| epoch  13 |  1550/ 1938 batches | ms/batch 114.32 | loss  2.43 | ppl    11.31\n",
            "| epoch  13 |  1600/ 1938 batches | ms/batch 114.46 | loss  2.44 | ppl    11.44\n",
            "| epoch  13 |  1650/ 1938 batches | ms/batch 113.96 | loss  2.43 | ppl    11.31\n",
            "| epoch  13 |  1700/ 1938 batches | ms/batch 114.59 | loss  2.42 | ppl    11.30\n",
            "| epoch  13 |  1750/ 1938 batches | ms/batch 114.79 | loss  2.43 | ppl    11.34\n",
            "| epoch  13 |  1800/ 1938 batches | ms/batch 114.72 | loss  2.43 | ppl    11.34\n",
            "| epoch  13 |  1850/ 1938 batches | ms/batch 114.72 | loss  2.43 | ppl    11.32\n",
            "| epoch  13 |  1900/ 1938 batches | ms/batch 114.80 | loss  2.42 | ppl    11.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 311.39s | train loss  1.32 | train ppl     3.75 | valid loss  1.35 | valid ppl     3.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 13) to best_model_shakespeare_lstm.pt with Val PPL: 3.85 **\n",
            "| epoch  14 |    50/ 1938 batches | ms/batch 135.21 | loss  2.46 | ppl    11.69\n",
            "| epoch  14 |   100/ 1938 batches | ms/batch 114.56 | loss  2.40 | ppl    11.07\n",
            "| epoch  14 |   150/ 1938 batches | ms/batch 114.82 | loss  2.42 | ppl    11.20\n",
            "| epoch  14 |   200/ 1938 batches | ms/batch 114.60 | loss  2.40 | ppl    11.07\n",
            "| epoch  14 |   250/ 1938 batches | ms/batch 114.89 | loss  2.41 | ppl    11.18\n",
            "| epoch  14 |   300/ 1938 batches | ms/batch 114.86 | loss  2.41 | ppl    11.16\n",
            "| epoch  14 |   350/ 1938 batches | ms/batch 115.22 | loss  2.42 | ppl    11.20\n",
            "| epoch  14 |   400/ 1938 batches | ms/batch 114.81 | loss  2.42 | ppl    11.23\n",
            "| epoch  14 |   450/ 1938 batches | ms/batch 114.84 | loss  2.41 | ppl    11.15\n",
            "| epoch  14 |   500/ 1938 batches | ms/batch 115.01 | loss  2.41 | ppl    11.12\n",
            "| epoch  14 |   550/ 1938 batches | ms/batch 115.37 | loss  2.42 | ppl    11.19\n",
            "| epoch  14 |   600/ 1938 batches | ms/batch 115.06 | loss  2.41 | ppl    11.17\n",
            "| epoch  14 |   650/ 1938 batches | ms/batch 115.37 | loss  2.42 | ppl    11.21\n",
            "| epoch  14 |   700/ 1938 batches | ms/batch 115.13 | loss  2.42 | ppl    11.20\n",
            "| epoch  14 |   750/ 1938 batches | ms/batch 115.09 | loss  2.42 | ppl    11.21\n",
            "| epoch  14 |   800/ 1938 batches | ms/batch 115.27 | loss  2.41 | ppl    11.08\n",
            "| epoch  14 |   850/ 1938 batches | ms/batch 115.35 | loss  2.41 | ppl    11.14\n",
            "| epoch  14 |   900/ 1938 batches | ms/batch 115.32 | loss  2.41 | ppl    11.14\n",
            "| epoch  14 |   950/ 1938 batches | ms/batch 115.29 | loss  2.41 | ppl    11.11\n",
            "| epoch  14 |  1000/ 1938 batches | ms/batch 115.04 | loss  2.41 | ppl    11.09\n",
            "| epoch  14 |  1050/ 1938 batches | ms/batch 115.07 | loss  2.41 | ppl    11.15\n",
            "| epoch  14 |  1100/ 1938 batches | ms/batch 114.78 | loss  2.40 | ppl    11.06\n",
            "| epoch  14 |  1150/ 1938 batches | ms/batch 115.02 | loss  2.41 | ppl    11.10\n",
            "| epoch  14 |  1200/ 1938 batches | ms/batch 114.71 | loss  2.41 | ppl    11.11\n",
            "| epoch  14 |  1250/ 1938 batches | ms/batch 114.90 | loss  2.41 | ppl    11.16\n",
            "| epoch  14 |  1300/ 1938 batches | ms/batch 114.42 | loss  2.40 | ppl    11.03\n",
            "| epoch  14 |  1350/ 1938 batches | ms/batch 114.73 | loss  2.41 | ppl    11.10\n",
            "| epoch  14 |  1400/ 1938 batches | ms/batch 114.79 | loss  2.41 | ppl    11.09\n",
            "| epoch  14 |  1450/ 1938 batches | ms/batch 114.76 | loss  2.40 | ppl    11.07\n",
            "| epoch  14 |  1500/ 1938 batches | ms/batch 114.69 | loss  2.40 | ppl    11.04\n",
            "| epoch  14 |  1550/ 1938 batches | ms/batch 114.79 | loss  2.41 | ppl    11.13\n",
            "| epoch  14 |  1600/ 1938 batches | ms/batch 114.39 | loss  2.40 | ppl    11.05\n",
            "| epoch  14 |  1650/ 1938 batches | ms/batch 114.47 | loss  2.41 | ppl    11.10\n",
            "| epoch  14 |  1700/ 1938 batches | ms/batch 114.57 | loss  2.40 | ppl    11.02\n",
            "| epoch  14 |  1750/ 1938 batches | ms/batch 114.39 | loss  2.40 | ppl    11.04\n",
            "| epoch  14 |  1800/ 1938 batches | ms/batch 114.85 | loss  2.41 | ppl    11.10\n",
            "| epoch  14 |  1850/ 1938 batches | ms/batch 114.42 | loss  2.39 | ppl    10.95\n",
            "| epoch  14 |  1900/ 1938 batches | ms/batch 114.66 | loss  2.39 | ppl    10.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 311.75s | train loss  1.29 | train ppl     3.63 | valid loss  1.32 | valid ppl     3.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 14) to best_model_shakespeare_lstm.pt with Val PPL: 3.74 **\n",
            "| epoch  15 |    50/ 1938 batches | ms/batch 135.24 | loss  2.43 | ppl    11.36\n",
            "| epoch  15 |   100/ 1938 batches | ms/batch 114.92 | loss  2.39 | ppl    10.87\n",
            "| epoch  15 |   150/ 1938 batches | ms/batch 115.18 | loss  2.39 | ppl    10.92\n",
            "| epoch  15 |   200/ 1938 batches | ms/batch 114.94 | loss  2.39 | ppl    10.88\n",
            "| epoch  15 |   250/ 1938 batches | ms/batch 115.07 | loss  2.39 | ppl    10.94\n",
            "| epoch  15 |   300/ 1938 batches | ms/batch 115.15 | loss  2.38 | ppl    10.82\n",
            "| epoch  15 |   350/ 1938 batches | ms/batch 115.42 | loss  2.39 | ppl    10.89\n",
            "| epoch  15 |   400/ 1938 batches | ms/batch 115.40 | loss  2.38 | ppl    10.83\n",
            "| epoch  15 |   450/ 1938 batches | ms/batch 115.42 | loss  2.39 | ppl    10.92\n",
            "| epoch  15 |   500/ 1938 batches | ms/batch 115.46 | loss  2.38 | ppl    10.86\n",
            "| epoch  15 |   550/ 1938 batches | ms/batch 115.59 | loss  2.39 | ppl    10.92\n",
            "| epoch  15 |   600/ 1938 batches | ms/batch 115.43 | loss  2.38 | ppl    10.82\n",
            "| epoch  15 |   650/ 1938 batches | ms/batch 115.49 | loss  2.38 | ppl    10.83\n",
            "| epoch  15 |   700/ 1938 batches | ms/batch 114.82 | loss  2.38 | ppl    10.82\n",
            "| epoch  15 |   750/ 1938 batches | ms/batch 114.72 | loss  2.38 | ppl    10.84\n",
            "| epoch  15 |   800/ 1938 batches | ms/batch 114.94 | loss  2.38 | ppl    10.81\n",
            "| epoch  15 |   850/ 1938 batches | ms/batch 114.70 | loss  2.39 | ppl    10.86\n",
            "| epoch  15 |   900/ 1938 batches | ms/batch 114.85 | loss  2.38 | ppl    10.82\n",
            "| epoch  15 |   950/ 1938 batches | ms/batch 114.91 | loss  2.38 | ppl    10.79\n",
            "| epoch  15 |  1000/ 1938 batches | ms/batch 114.56 | loss  2.38 | ppl    10.84\n",
            "| epoch  15 |  1050/ 1938 batches | ms/batch 114.66 | loss  2.38 | ppl    10.84\n",
            "| epoch  15 |  1100/ 1938 batches | ms/batch 114.73 | loss  2.38 | ppl    10.82\n",
            "| epoch  15 |  1150/ 1938 batches | ms/batch 114.75 | loss  2.39 | ppl    10.89\n",
            "| epoch  15 |  1200/ 1938 batches | ms/batch 114.52 | loss  2.38 | ppl    10.79\n",
            "| epoch  15 |  1250/ 1938 batches | ms/batch 114.64 | loss  2.38 | ppl    10.84\n",
            "| epoch  15 |  1300/ 1938 batches | ms/batch 114.42 | loss  2.38 | ppl    10.77\n",
            "| epoch  15 |  1350/ 1938 batches | ms/batch 114.41 | loss  2.38 | ppl    10.79\n",
            "| epoch  15 |  1400/ 1938 batches | ms/batch 114.45 | loss  2.38 | ppl    10.78\n",
            "| epoch  15 |  1450/ 1938 batches | ms/batch 114.24 | loss  2.38 | ppl    10.79\n",
            "| epoch  15 |  1500/ 1938 batches | ms/batch 114.42 | loss  2.37 | ppl    10.69\n",
            "| epoch  15 |  1550/ 1938 batches | ms/batch 114.50 | loss  2.37 | ppl    10.74\n",
            "| epoch  15 |  1600/ 1938 batches | ms/batch 114.80 | loss  2.38 | ppl    10.79\n",
            "| epoch  15 |  1650/ 1938 batches | ms/batch 114.29 | loss  2.38 | ppl    10.82\n",
            "| epoch  15 |  1700/ 1938 batches | ms/batch 114.63 | loss  2.38 | ppl    10.81\n",
            "| epoch  15 |  1750/ 1938 batches | ms/batch 114.64 | loss  2.37 | ppl    10.74\n",
            "| epoch  15 |  1800/ 1938 batches | ms/batch 114.84 | loss  2.38 | ppl    10.79\n",
            "| epoch  15 |  1850/ 1938 batches | ms/batch 114.87 | loss  2.38 | ppl    10.77\n",
            "| epoch  15 |  1900/ 1938 batches | ms/batch 114.88 | loss  2.37 | ppl    10.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 311.74s | train loss  1.26 | train ppl     3.53 | valid loss  1.29 | valid ppl     3.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 15) to best_model_shakespeare_lstm.pt with Val PPL: 3.63 **\n",
            "| epoch  16 |    50/ 1938 batches | ms/batch 135.07 | loss  2.39 | ppl    10.97\n",
            "| epoch  16 |   100/ 1938 batches | ms/batch 114.59 | loss  2.36 | ppl    10.60\n",
            "| epoch  16 |   150/ 1938 batches | ms/batch 114.39 | loss  2.36 | ppl    10.62\n",
            "| epoch  16 |   200/ 1938 batches | ms/batch 114.51 | loss  2.36 | ppl    10.62\n",
            "| epoch  16 |   250/ 1938 batches | ms/batch 114.88 | loss  2.36 | ppl    10.62\n",
            "| epoch  16 |   300/ 1938 batches | ms/batch 114.64 | loss  2.36 | ppl    10.59\n",
            "| epoch  16 |   350/ 1938 batches | ms/batch 115.23 | loss  2.36 | ppl    10.63\n",
            "| epoch  16 |   400/ 1938 batches | ms/batch 115.01 | loss  2.36 | ppl    10.62\n",
            "| epoch  16 |   450/ 1938 batches | ms/batch 115.08 | loss  2.36 | ppl    10.59\n",
            "| epoch  16 |   500/ 1938 batches | ms/batch 114.94 | loss  2.37 | ppl    10.66\n",
            "| epoch  16 |   550/ 1938 batches | ms/batch 115.18 | loss  2.37 | ppl    10.66\n",
            "| epoch  16 |   600/ 1938 batches | ms/batch 115.09 | loss  2.36 | ppl    10.58\n",
            "| epoch  16 |   650/ 1938 batches | ms/batch 115.54 | loss  2.35 | ppl    10.47\n",
            "| epoch  16 |   700/ 1938 batches | ms/batch 115.22 | loss  2.37 | ppl    10.65\n",
            "| epoch  16 |   750/ 1938 batches | ms/batch 115.18 | loss  2.36 | ppl    10.56\n",
            "| epoch  16 |   800/ 1938 batches | ms/batch 115.08 | loss  2.35 | ppl    10.53\n",
            "| epoch  16 |   850/ 1938 batches | ms/batch 115.34 | loss  2.36 | ppl    10.63\n",
            "| epoch  16 |   900/ 1938 batches | ms/batch 115.23 | loss  2.36 | ppl    10.58\n",
            "| epoch  16 |   950/ 1938 batches | ms/batch 115.53 | loss  2.36 | ppl    10.57\n",
            "| epoch  16 |  1000/ 1938 batches | ms/batch 115.46 | loss  2.36 | ppl    10.60\n",
            "| epoch  16 |  1050/ 1938 batches | ms/batch 115.42 | loss  2.36 | ppl    10.55\n",
            "| epoch  16 |  1100/ 1938 batches | ms/batch 115.00 | loss  2.36 | ppl    10.59\n",
            "| epoch  16 |  1150/ 1938 batches | ms/batch 115.35 | loss  2.36 | ppl    10.57\n",
            "| epoch  16 |  1200/ 1938 batches | ms/batch 115.14 | loss  2.36 | ppl    10.54\n",
            "| epoch  16 |  1250/ 1938 batches | ms/batch 115.21 | loss  2.35 | ppl    10.51\n",
            "| epoch  16 |  1300/ 1938 batches | ms/batch 115.21 | loss  2.36 | ppl    10.58\n",
            "| epoch  16 |  1350/ 1938 batches | ms/batch 114.76 | loss  2.35 | ppl    10.54\n",
            "| epoch  16 |  1400/ 1938 batches | ms/batch 115.14 | loss  2.35 | ppl    10.54\n",
            "| epoch  16 |  1450/ 1938 batches | ms/batch 114.72 | loss  2.36 | ppl    10.58\n",
            "| epoch  16 |  1500/ 1938 batches | ms/batch 115.12 | loss  2.35 | ppl    10.47\n",
            "| epoch  16 |  1550/ 1938 batches | ms/batch 114.70 | loss  2.35 | ppl    10.52\n",
            "| epoch  16 |  1600/ 1938 batches | ms/batch 114.96 | loss  2.35 | ppl    10.49\n",
            "| epoch  16 |  1650/ 1938 batches | ms/batch 114.36 | loss  2.35 | ppl    10.50\n",
            "| epoch  16 |  1700/ 1938 batches | ms/batch 114.90 | loss  2.34 | ppl    10.40\n",
            "| epoch  16 |  1750/ 1938 batches | ms/batch 114.46 | loss  2.35 | ppl    10.47\n",
            "| epoch  16 |  1800/ 1938 batches | ms/batch 114.89 | loss  2.36 | ppl    10.58\n",
            "| epoch  16 |  1850/ 1938 batches | ms/batch 114.76 | loss  2.35 | ppl    10.44\n",
            "| epoch  16 |  1900/ 1938 batches | ms/batch 114.69 | loss  2.35 | ppl    10.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 311.98s | train loss  1.23 | train ppl     3.44 | valid loss  1.26 | valid ppl     3.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 16) to best_model_shakespeare_lstm.pt with Val PPL: 3.53 **\n",
            "| epoch  17 |    50/ 1938 batches | ms/batch 135.44 | loss  2.38 | ppl    10.78\n",
            "| epoch  17 |   100/ 1938 batches | ms/batch 114.77 | loss  2.33 | ppl    10.27\n",
            "| epoch  17 |   150/ 1938 batches | ms/batch 114.54 | loss  2.34 | ppl    10.33\n",
            "| epoch  17 |   200/ 1938 batches | ms/batch 114.46 | loss  2.33 | ppl    10.30\n",
            "| epoch  17 |   250/ 1938 batches | ms/batch 114.86 | loss  2.34 | ppl    10.40\n",
            "| epoch  17 |   300/ 1938 batches | ms/batch 114.80 | loss  2.34 | ppl    10.40\n",
            "| epoch  17 |   350/ 1938 batches | ms/batch 114.86 | loss  2.34 | ppl    10.39\n",
            "| epoch  17 |   400/ 1938 batches | ms/batch 114.71 | loss  2.34 | ppl    10.33\n",
            "| epoch  17 |   450/ 1938 batches | ms/batch 115.28 | loss  2.34 | ppl    10.37\n",
            "| epoch  17 |   500/ 1938 batches | ms/batch 114.94 | loss  2.34 | ppl    10.36\n",
            "| epoch  17 |   550/ 1938 batches | ms/batch 115.41 | loss  2.34 | ppl    10.36\n",
            "| epoch  17 |   600/ 1938 batches | ms/batch 115.23 | loss  2.35 | ppl    10.46\n",
            "| epoch  17 |   650/ 1938 batches | ms/batch 115.23 | loss  2.33 | ppl    10.26\n",
            "| epoch  17 |   700/ 1938 batches | ms/batch 115.32 | loss  2.34 | ppl    10.38\n",
            "| epoch  17 |   750/ 1938 batches | ms/batch 115.22 | loss  2.34 | ppl    10.34\n",
            "| epoch  17 |   800/ 1938 batches | ms/batch 114.76 | loss  2.34 | ppl    10.43\n",
            "| epoch  17 |   850/ 1938 batches | ms/batch 115.03 | loss  2.34 | ppl    10.43\n",
            "| epoch  17 |   900/ 1938 batches | ms/batch 115.08 | loss  2.34 | ppl    10.36\n",
            "| epoch  17 |   950/ 1938 batches | ms/batch 115.38 | loss  2.34 | ppl    10.34\n",
            "| epoch  17 |  1000/ 1938 batches | ms/batch 115.32 | loss  2.34 | ppl    10.34\n",
            "| epoch  17 |  1050/ 1938 batches | ms/batch 115.52 | loss  2.34 | ppl    10.34\n",
            "| epoch  17 |  1100/ 1938 batches | ms/batch 115.26 | loss  2.33 | ppl    10.33\n",
            "| epoch  17 |  1150/ 1938 batches | ms/batch 115.41 | loss  2.33 | ppl    10.29\n",
            "| epoch  17 |  1200/ 1938 batches | ms/batch 115.34 | loss  2.34 | ppl    10.35\n",
            "| epoch  17 |  1250/ 1938 batches | ms/batch 115.30 | loss  2.33 | ppl    10.30\n",
            "| epoch  17 |  1300/ 1938 batches | ms/batch 115.50 | loss  2.33 | ppl    10.26\n",
            "| epoch  17 |  1350/ 1938 batches | ms/batch 115.17 | loss  2.34 | ppl    10.34\n",
            "| epoch  17 |  1400/ 1938 batches | ms/batch 115.04 | loss  2.34 | ppl    10.33\n",
            "| epoch  17 |  1450/ 1938 batches | ms/batch 114.80 | loss  2.33 | ppl    10.31\n",
            "| epoch  17 |  1500/ 1938 batches | ms/batch 114.86 | loss  2.33 | ppl    10.28\n",
            "| epoch  17 |  1550/ 1938 batches | ms/batch 114.65 | loss  2.33 | ppl    10.28\n",
            "| epoch  17 |  1600/ 1938 batches | ms/batch 115.20 | loss  2.32 | ppl    10.20\n",
            "| epoch  17 |  1650/ 1938 batches | ms/batch 114.70 | loss  2.33 | ppl    10.29\n",
            "| epoch  17 |  1700/ 1938 batches | ms/batch 114.86 | loss  2.33 | ppl    10.23\n",
            "| epoch  17 |  1750/ 1938 batches | ms/batch 114.42 | loss  2.33 | ppl    10.25\n",
            "| epoch  17 |  1800/ 1938 batches | ms/batch 114.91 | loss  2.33 | ppl    10.24\n",
            "| epoch  17 |  1850/ 1938 batches | ms/batch 114.67 | loss  2.32 | ppl    10.20\n",
            "| epoch  17 |  1900/ 1938 batches | ms/batch 114.98 | loss  2.32 | ppl    10.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 312.08s | train loss  1.21 | train ppl     3.35 | valid loss  1.24 | valid ppl     3.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 17) to best_model_shakespeare_lstm.pt with Val PPL: 3.44 **\n",
            "| epoch  18 |    50/ 1938 batches | ms/batch 125.52 | loss  2.35 | ppl    10.51\n",
            "| epoch  18 |   100/ 1938 batches | ms/batch 114.96 | loss  2.31 | ppl    10.06\n",
            "| epoch  18 |   150/ 1938 batches | ms/batch 114.93 | loss  2.31 | ppl    10.06\n",
            "| epoch  18 |   200/ 1938 batches | ms/batch 114.65 | loss  2.31 | ppl    10.11\n",
            "| epoch  18 |   250/ 1938 batches | ms/batch 114.88 | loss  2.32 | ppl    10.14\n",
            "| epoch  18 |   300/ 1938 batches | ms/batch 114.54 | loss  2.31 | ppl    10.10\n",
            "| epoch  18 |   350/ 1938 batches | ms/batch 114.77 | loss  2.32 | ppl    10.13\n",
            "| epoch  18 |   400/ 1938 batches | ms/batch 114.20 | loss  2.32 | ppl    10.17\n",
            "| epoch  18 |   450/ 1938 batches | ms/batch 114.60 | loss  2.31 | ppl    10.12\n",
            "| epoch  18 |   500/ 1938 batches | ms/batch 114.52 | loss  2.31 | ppl    10.11\n",
            "| epoch  18 |   550/ 1938 batches | ms/batch 114.44 | loss  2.32 | ppl    10.17\n",
            "| epoch  18 |   600/ 1938 batches | ms/batch 114.32 | loss  2.31 | ppl    10.12\n",
            "| epoch  18 |   650/ 1938 batches | ms/batch 114.32 | loss  2.32 | ppl    10.14\n",
            "| epoch  18 |   700/ 1938 batches | ms/batch 114.46 | loss  2.31 | ppl    10.06\n",
            "| epoch  18 |   750/ 1938 batches | ms/batch 114.62 | loss  2.32 | ppl    10.16\n",
            "| epoch  18 |   800/ 1938 batches | ms/batch 114.47 | loss  2.32 | ppl    10.19\n",
            "| epoch  18 |   850/ 1938 batches | ms/batch 114.54 | loss  2.32 | ppl    10.17\n",
            "| epoch  18 |   900/ 1938 batches | ms/batch 114.23 | loss  2.31 | ppl    10.09\n",
            "| epoch  18 |   950/ 1938 batches | ms/batch 114.60 | loss  2.31 | ppl    10.09\n",
            "| epoch  18 |  1000/ 1938 batches | ms/batch 114.45 | loss  2.31 | ppl    10.09\n",
            "| epoch  18 |  1050/ 1938 batches | ms/batch 114.44 | loss  2.31 | ppl    10.05\n",
            "| epoch  18 |  1100/ 1938 batches | ms/batch 114.64 | loss  2.31 | ppl    10.08\n",
            "| epoch  18 |  1150/ 1938 batches | ms/batch 114.30 | loss  2.31 | ppl    10.08\n",
            "| epoch  18 |  1200/ 1938 batches | ms/batch 114.61 | loss  2.31 | ppl    10.10\n",
            "| epoch  18 |  1250/ 1938 batches | ms/batch 114.53 | loss  2.31 | ppl    10.07\n",
            "| epoch  18 |  1300/ 1938 batches | ms/batch 114.55 | loss  2.31 | ppl    10.11\n",
            "| epoch  18 |  1350/ 1938 batches | ms/batch 114.47 | loss  2.32 | ppl    10.16\n",
            "| epoch  18 |  1400/ 1938 batches | ms/batch 114.94 | loss  2.31 | ppl    10.09\n",
            "| epoch  18 |  1450/ 1938 batches | ms/batch 114.07 | loss  2.31 | ppl    10.06\n",
            "| epoch  18 |  1500/ 1938 batches | ms/batch 114.27 | loss  2.31 | ppl    10.06\n",
            "| epoch  18 |  1550/ 1938 batches | ms/batch 114.13 | loss  2.31 | ppl    10.05\n",
            "| epoch  18 |  1600/ 1938 batches | ms/batch 114.44 | loss  2.31 | ppl    10.07\n",
            "| epoch  18 |  1650/ 1938 batches | ms/batch 114.45 | loss  2.30 | ppl    10.01\n",
            "| epoch  18 |  1700/ 1938 batches | ms/batch 114.58 | loss  2.31 | ppl    10.03\n",
            "| epoch  18 |  1750/ 1938 batches | ms/batch 114.14 | loss  2.31 | ppl    10.10\n",
            "| epoch  18 |  1800/ 1938 batches | ms/batch 114.57 | loss  2.31 | ppl    10.04\n",
            "| epoch  18 |  1850/ 1938 batches | ms/batch 114.22 | loss  2.30 | ppl    10.02\n",
            "| epoch  18 |  1900/ 1938 batches | ms/batch 114.72 | loss  2.30 | ppl     9.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 310.91s | train loss  1.19 | train ppl     3.28 | valid loss  1.21 | valid ppl     3.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 18) to best_model_shakespeare_lstm.pt with Val PPL: 3.37 **\n",
            "| epoch  19 |    50/ 1938 batches | ms/batch 122.50 | loss  2.34 | ppl    10.36\n",
            "| epoch  19 |   100/ 1938 batches | ms/batch 114.94 | loss  2.29 | ppl     9.91\n",
            "| epoch  19 |   150/ 1938 batches | ms/batch 115.18 | loss  2.30 | ppl    10.00\n",
            "| epoch  19 |   200/ 1938 batches | ms/batch 115.00 | loss  2.30 | ppl     9.97\n",
            "| epoch  19 |   250/ 1938 batches | ms/batch 115.34 | loss  2.29 | ppl     9.92\n",
            "| epoch  19 |   300/ 1938 batches | ms/batch 114.80 | loss  2.29 | ppl     9.90\n",
            "| epoch  19 |   350/ 1938 batches | ms/batch 114.99 | loss  2.30 | ppl     9.93\n",
            "| epoch  19 |   400/ 1938 batches | ms/batch 115.07 | loss  2.29 | ppl     9.92\n",
            "| epoch  19 |   450/ 1938 batches | ms/batch 115.13 | loss  2.29 | ppl     9.91\n",
            "| epoch  19 |   500/ 1938 batches | ms/batch 115.10 | loss  2.29 | ppl     9.88\n",
            "| epoch  19 |   550/ 1938 batches | ms/batch 115.01 | loss  2.30 | ppl     9.94\n",
            "| epoch  19 |   600/ 1938 batches | ms/batch 114.65 | loss  2.29 | ppl     9.87\n",
            "| epoch  19 |   650/ 1938 batches | ms/batch 114.81 | loss  2.30 | ppl     9.93\n",
            "| epoch  19 |   700/ 1938 batches | ms/batch 114.51 | loss  2.29 | ppl     9.88\n",
            "| epoch  19 |   750/ 1938 batches | ms/batch 114.68 | loss  2.29 | ppl     9.91\n",
            "| epoch  19 |   800/ 1938 batches | ms/batch 114.76 | loss  2.29 | ppl     9.92\n",
            "| epoch  19 |   850/ 1938 batches | ms/batch 114.71 | loss  2.30 | ppl     9.96\n",
            "| epoch  19 |   900/ 1938 batches | ms/batch 114.94 | loss  2.29 | ppl     9.90\n",
            "| epoch  19 |   950/ 1938 batches | ms/batch 114.54 | loss  2.29 | ppl     9.83\n",
            "| epoch  19 |  1000/ 1938 batches | ms/batch 114.93 | loss  2.29 | ppl     9.89\n",
            "| epoch  19 |  1050/ 1938 batches | ms/batch 114.83 | loss  2.29 | ppl     9.89\n",
            "| epoch  19 |  1100/ 1938 batches | ms/batch 114.82 | loss  2.29 | ppl     9.87\n",
            "| epoch  19 |  1150/ 1938 batches | ms/batch 114.61 | loss  2.29 | ppl     9.87\n",
            "| epoch  19 |  1200/ 1938 batches | ms/batch 114.73 | loss  2.29 | ppl     9.90\n",
            "| epoch  19 |  1250/ 1938 batches | ms/batch 114.60 | loss  2.29 | ppl     9.87\n",
            "| epoch  19 |  1300/ 1938 batches | ms/batch 114.86 | loss  2.28 | ppl     9.78\n",
            "| epoch  19 |  1350/ 1938 batches | ms/batch 114.81 | loss  2.29 | ppl     9.88\n",
            "| epoch  19 |  1400/ 1938 batches | ms/batch 114.76 | loss  2.29 | ppl     9.91\n",
            "| epoch  19 |  1450/ 1938 batches | ms/batch 114.53 | loss  2.29 | ppl     9.83\n",
            "| epoch  19 |  1500/ 1938 batches | ms/batch 114.94 | loss  2.29 | ppl     9.88\n",
            "| epoch  19 |  1550/ 1938 batches | ms/batch 114.54 | loss  2.28 | ppl     9.79\n",
            "| epoch  19 |  1600/ 1938 batches | ms/batch 114.80 | loss  2.29 | ppl     9.90\n",
            "| epoch  19 |  1650/ 1938 batches | ms/batch 114.63 | loss  2.29 | ppl     9.88\n",
            "| epoch  19 |  1700/ 1938 batches | ms/batch 114.79 | loss  2.29 | ppl     9.88\n",
            "| epoch  19 |  1750/ 1938 batches | ms/batch 114.43 | loss  2.29 | ppl     9.89\n",
            "| epoch  19 |  1800/ 1938 batches | ms/batch 114.48 | loss  2.29 | ppl     9.88\n",
            "| epoch  19 |  1850/ 1938 batches | ms/batch 114.62 | loss  2.28 | ppl     9.82\n",
            "| epoch  19 |  1900/ 1938 batches | ms/batch 114.46 | loss  2.29 | ppl     9.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 310.88s | train loss  1.17 | train ppl     3.21 | valid loss  1.19 | valid ppl     3.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 19) to best_model_shakespeare_lstm.pt with Val PPL: 3.30 **\n",
            "| epoch  20 |    50/ 1938 batches | ms/batch 129.74 | loss  2.32 | ppl    10.14\n",
            "| epoch  20 |   100/ 1938 batches | ms/batch 114.46 | loss  2.27 | ppl     9.64\n",
            "| epoch  20 |   150/ 1938 batches | ms/batch 114.72 | loss  2.27 | ppl     9.64\n",
            "| epoch  20 |   200/ 1938 batches | ms/batch 114.87 | loss  2.27 | ppl     9.70\n",
            "| epoch  20 |   250/ 1938 batches | ms/batch 114.92 | loss  2.28 | ppl     9.74\n",
            "| epoch  20 |   300/ 1938 batches | ms/batch 114.70 | loss  2.28 | ppl     9.75\n",
            "| epoch  20 |   350/ 1938 batches | ms/batch 114.88 | loss  2.27 | ppl     9.71\n",
            "| epoch  20 |   400/ 1938 batches | ms/batch 114.69 | loss  2.28 | ppl     9.76\n",
            "| epoch  20 |   450/ 1938 batches | ms/batch 114.94 | loss  2.27 | ppl     9.70\n",
            "| epoch  20 |   500/ 1938 batches | ms/batch 115.17 | loss  2.28 | ppl     9.75\n",
            "| epoch  20 |   550/ 1938 batches | ms/batch 115.34 | loss  2.27 | ppl     9.70\n",
            "| epoch  20 |   600/ 1938 batches | ms/batch 115.42 | loss  2.28 | ppl     9.81\n",
            "| epoch  20 |   650/ 1938 batches | ms/batch 115.09 | loss  2.28 | ppl     9.74\n",
            "| epoch  20 |   700/ 1938 batches | ms/batch 115.19 | loss  2.27 | ppl     9.71\n",
            "| epoch  20 |   750/ 1938 batches | ms/batch 115.31 | loss  2.28 | ppl     9.76\n",
            "| epoch  20 |   800/ 1938 batches | ms/batch 115.53 | loss  2.27 | ppl     9.69\n",
            "| epoch  20 |   850/ 1938 batches | ms/batch 115.32 | loss  2.27 | ppl     9.72\n",
            "| epoch  20 |   900/ 1938 batches | ms/batch 115.11 | loss  2.28 | ppl     9.77\n",
            "| epoch  20 |   950/ 1938 batches | ms/batch 114.70 | loss  2.26 | ppl     9.62\n",
            "| epoch  20 |  1000/ 1938 batches | ms/batch 114.79 | loss  2.28 | ppl     9.74\n",
            "| epoch  20 |  1050/ 1938 batches | ms/batch 114.78 | loss  2.28 | ppl     9.75\n",
            "| epoch  20 |  1100/ 1938 batches | ms/batch 114.75 | loss  2.27 | ppl     9.68\n",
            "| epoch  20 |  1150/ 1938 batches | ms/batch 114.69 | loss  2.27 | ppl     9.73\n",
            "| epoch  20 |  1200/ 1938 batches | ms/batch 114.75 | loss  2.27 | ppl     9.71\n",
            "| epoch  20 |  1250/ 1938 batches | ms/batch 114.53 | loss  2.27 | ppl     9.66\n",
            "| epoch  20 |  1300/ 1938 batches | ms/batch 114.64 | loss  2.26 | ppl     9.61\n",
            "| epoch  20 |  1350/ 1938 batches | ms/batch 114.10 | loss  2.28 | ppl     9.73\n",
            "| epoch  20 |  1400/ 1938 batches | ms/batch 114.76 | loss  2.27 | ppl     9.69\n",
            "| epoch  20 |  1450/ 1938 batches | ms/batch 114.58 | loss  2.28 | ppl     9.75\n",
            "| epoch  20 |  1500/ 1938 batches | ms/batch 114.61 | loss  2.27 | ppl     9.69\n",
            "| epoch  20 |  1550/ 1938 batches | ms/batch 114.31 | loss  2.26 | ppl     9.63\n",
            "| epoch  20 |  1600/ 1938 batches | ms/batch 114.57 | loss  2.27 | ppl     9.68\n",
            "| epoch  20 |  1650/ 1938 batches | ms/batch 114.24 | loss  2.26 | ppl     9.61\n",
            "| epoch  20 |  1700/ 1938 batches | ms/batch 114.58 | loss  2.27 | ppl     9.63\n",
            "| epoch  20 |  1750/ 1938 batches | ms/batch 114.74 | loss  2.27 | ppl     9.71\n",
            "| epoch  20 |  1800/ 1938 batches | ms/batch 114.82 | loss  2.26 | ppl     9.63\n",
            "| epoch  20 |  1850/ 1938 batches | ms/batch 114.89 | loss  2.27 | ppl     9.66\n",
            "| epoch  20 |  1900/ 1938 batches | ms/batch 114.72 | loss  2.27 | ppl     9.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 311.51s | train loss  1.15 | train ppl     3.15 | valid loss  1.18 | valid ppl     3.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 20) to best_model_shakespeare_lstm.pt with Val PPL: 3.24 **\n",
            "| epoch  21 |    50/ 1938 batches | ms/batch 129.63 | loss  2.29 | ppl     9.91\n",
            "| epoch  21 |   100/ 1938 batches | ms/batch 114.35 | loss  2.25 | ppl     9.52\n",
            "| epoch  21 |   150/ 1938 batches | ms/batch 114.61 | loss  2.26 | ppl     9.55\n",
            "| epoch  21 |   200/ 1938 batches | ms/batch 114.73 | loss  2.25 | ppl     9.51\n",
            "| epoch  21 |   250/ 1938 batches | ms/batch 115.11 | loss  2.26 | ppl     9.58\n",
            "| epoch  21 |   300/ 1938 batches | ms/batch 114.97 | loss  2.26 | ppl     9.59\n",
            "| epoch  21 |   350/ 1938 batches | ms/batch 115.19 | loss  2.26 | ppl     9.57\n",
            "| epoch  21 |   400/ 1938 batches | ms/batch 115.03 | loss  2.26 | ppl     9.55\n",
            "| epoch  21 |   450/ 1938 batches | ms/batch 115.15 | loss  2.26 | ppl     9.60\n",
            "| epoch  21 |   500/ 1938 batches | ms/batch 115.40 | loss  2.25 | ppl     9.51\n",
            "| epoch  21 |   550/ 1938 batches | ms/batch 115.17 | loss  2.26 | ppl     9.59\n",
            "| epoch  21 |   600/ 1938 batches | ms/batch 115.21 | loss  2.25 | ppl     9.52\n",
            "| epoch  21 |   650/ 1938 batches | ms/batch 115.22 | loss  2.26 | ppl     9.59\n",
            "| epoch  21 |   700/ 1938 batches | ms/batch 115.04 | loss  2.25 | ppl     9.53\n",
            "| epoch  21 |   750/ 1938 batches | ms/batch 114.69 | loss  2.26 | ppl     9.58\n",
            "| epoch  21 |   800/ 1938 batches | ms/batch 115.03 | loss  2.26 | ppl     9.60\n",
            "| epoch  21 |   850/ 1938 batches | ms/batch 114.77 | loss  2.25 | ppl     9.52\n",
            "| epoch  21 |   900/ 1938 batches | ms/batch 114.96 | loss  2.26 | ppl     9.56\n",
            "| epoch  21 |   950/ 1938 batches | ms/batch 114.66 | loss  2.26 | ppl     9.56\n",
            "| epoch  21 |  1000/ 1938 batches | ms/batch 114.67 | loss  2.25 | ppl     9.50\n",
            "| epoch  21 |  1050/ 1938 batches | ms/batch 114.34 | loss  2.26 | ppl     9.55\n",
            "| epoch  21 |  1100/ 1938 batches | ms/batch 114.92 | loss  2.25 | ppl     9.53\n",
            "| epoch  21 |  1150/ 1938 batches | ms/batch 114.60 | loss  2.26 | ppl     9.56\n",
            "| epoch  21 |  1200/ 1938 batches | ms/batch 114.87 | loss  2.26 | ppl     9.58\n",
            "| epoch  21 |  1250/ 1938 batches | ms/batch 114.61 | loss  2.25 | ppl     9.50\n",
            "| epoch  21 |  1300/ 1938 batches | ms/batch 114.93 | loss  2.26 | ppl     9.58\n",
            "| epoch  21 |  1350/ 1938 batches | ms/batch 114.45 | loss  2.26 | ppl     9.54\n",
            "| epoch  21 |  1400/ 1938 batches | ms/batch 114.65 | loss  2.26 | ppl     9.54\n",
            "| epoch  21 |  1450/ 1938 batches | ms/batch 114.56 | loss  2.25 | ppl     9.45\n",
            "| epoch  21 |  1500/ 1938 batches | ms/batch 114.71 | loss  2.26 | ppl     9.54\n",
            "| epoch  21 |  1550/ 1938 batches | ms/batch 114.71 | loss  2.26 | ppl     9.57\n",
            "| epoch  21 |  1600/ 1938 batches | ms/batch 114.35 | loss  2.25 | ppl     9.49\n",
            "| epoch  21 |  1650/ 1938 batches | ms/batch 114.16 | loss  2.25 | ppl     9.44\n",
            "| epoch  21 |  1700/ 1938 batches | ms/batch 114.58 | loss  2.25 | ppl     9.50\n",
            "| epoch  21 |  1750/ 1938 batches | ms/batch 114.35 | loss  2.25 | ppl     9.49\n",
            "| epoch  21 |  1800/ 1938 batches | ms/batch 114.27 | loss  2.26 | ppl     9.54\n",
            "| epoch  21 |  1850/ 1938 batches | ms/batch 114.38 | loss  2.25 | ppl     9.49\n",
            "| epoch  21 |  1900/ 1938 batches | ms/batch 114.09 | loss  2.24 | ppl     9.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 311.41s | train loss  1.13 | train ppl     3.09 | valid loss  1.15 | valid ppl     3.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 21) to best_model_shakespeare_lstm.pt with Val PPL: 3.17 **\n",
            "| epoch  22 |    50/ 1938 batches | ms/batch 130.07 | loss  2.28 | ppl     9.75\n",
            "| epoch  22 |   100/ 1938 batches | ms/batch 114.64 | loss  2.24 | ppl     9.38\n",
            "| epoch  22 |   150/ 1938 batches | ms/batch 114.59 | loss  2.25 | ppl     9.46\n",
            "| epoch  22 |   200/ 1938 batches | ms/batch 114.44 | loss  2.24 | ppl     9.42\n",
            "| epoch  22 |   250/ 1938 batches | ms/batch 114.78 | loss  2.25 | ppl     9.47\n",
            "| epoch  22 |   300/ 1938 batches | ms/batch 114.50 | loss  2.24 | ppl     9.37\n",
            "| epoch  22 |   350/ 1938 batches | ms/batch 114.50 | loss  2.24 | ppl     9.41\n",
            "| epoch  22 |   400/ 1938 batches | ms/batch 114.30 | loss  2.24 | ppl     9.41\n",
            "| epoch  22 |   450/ 1938 batches | ms/batch 114.53 | loss  2.25 | ppl     9.46\n",
            "| epoch  22 |   500/ 1938 batches | ms/batch 114.45 | loss  2.24 | ppl     9.39\n",
            "| epoch  22 |   550/ 1938 batches | ms/batch 114.34 | loss  2.24 | ppl     9.40\n",
            "| epoch  22 |   600/ 1938 batches | ms/batch 114.64 | loss  2.23 | ppl     9.32\n",
            "| epoch  22 |   650/ 1938 batches | ms/batch 114.22 | loss  2.24 | ppl     9.39\n",
            "| epoch  22 |   700/ 1938 batches | ms/batch 114.22 | loss  2.24 | ppl     9.35\n",
            "| epoch  22 |   750/ 1938 batches | ms/batch 114.31 | loss  2.24 | ppl     9.40\n",
            "| epoch  22 |   800/ 1938 batches | ms/batch 114.43 | loss  2.24 | ppl     9.43\n",
            "| epoch  22 |   850/ 1938 batches | ms/batch 114.36 | loss  2.24 | ppl     9.42\n",
            "| epoch  22 |   900/ 1938 batches | ms/batch 114.83 | loss  2.24 | ppl     9.41\n",
            "| epoch  22 |   950/ 1938 batches | ms/batch 114.65 | loss  2.24 | ppl     9.40\n",
            "| epoch  22 |  1000/ 1938 batches | ms/batch 114.90 | loss  2.24 | ppl     9.44\n",
            "| epoch  22 |  1050/ 1938 batches | ms/batch 114.52 | loss  2.24 | ppl     9.38\n",
            "| epoch  22 |  1100/ 1938 batches | ms/batch 115.05 | loss  2.24 | ppl     9.41\n",
            "| epoch  22 |  1150/ 1938 batches | ms/batch 114.92 | loss  2.24 | ppl     9.42\n",
            "| epoch  22 |  1200/ 1938 batches | ms/batch 115.08 | loss  2.24 | ppl     9.39\n",
            "| epoch  22 |  1250/ 1938 batches | ms/batch 114.91 | loss  2.23 | ppl     9.31\n",
            "| epoch  22 |  1300/ 1938 batches | ms/batch 115.06 | loss  2.24 | ppl     9.37\n",
            "| epoch  22 |  1350/ 1938 batches | ms/batch 114.73 | loss  2.24 | ppl     9.39\n",
            "| epoch  22 |  1400/ 1938 batches | ms/batch 115.12 | loss  2.24 | ppl     9.39\n",
            "| epoch  22 |  1450/ 1938 batches | ms/batch 115.06 | loss  2.23 | ppl     9.34\n",
            "| epoch  22 |  1500/ 1938 batches | ms/batch 115.08 | loss  2.24 | ppl     9.41\n",
            "| epoch  22 |  1550/ 1938 batches | ms/batch 115.27 | loss  2.23 | ppl     9.31\n",
            "| epoch  22 |  1600/ 1938 batches | ms/batch 115.10 | loss  2.24 | ppl     9.38\n",
            "| epoch  22 |  1650/ 1938 batches | ms/batch 115.38 | loss  2.23 | ppl     9.30\n",
            "| epoch  22 |  1700/ 1938 batches | ms/batch 115.29 | loss  2.24 | ppl     9.36\n",
            "| epoch  22 |  1750/ 1938 batches | ms/batch 115.34 | loss  2.23 | ppl     9.34\n",
            "| epoch  22 |  1800/ 1938 batches | ms/batch 115.04 | loss  2.24 | ppl     9.40\n",
            "| epoch  22 |  1850/ 1938 batches | ms/batch 114.82 | loss  2.23 | ppl     9.34\n",
            "| epoch  22 |  1900/ 1938 batches | ms/batch 114.61 | loss  2.23 | ppl     9.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 311.49s | train loss  1.11 | train ppl     3.03 | valid loss  1.14 | valid ppl     3.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 22) to best_model_shakespeare_lstm.pt with Val PPL: 3.11 **\n",
            "| epoch  23 |    50/ 1938 batches | ms/batch 130.64 | loss  2.25 | ppl     9.53\n",
            "| epoch  23 |   100/ 1938 batches | ms/batch 114.94 | loss  2.22 | ppl     9.21\n",
            "| epoch  23 |   150/ 1938 batches | ms/batch 115.35 | loss  2.22 | ppl     9.18\n",
            "| epoch  23 |   200/ 1938 batches | ms/batch 115.22 | loss  2.22 | ppl     9.25\n",
            "| epoch  23 |   250/ 1938 batches | ms/batch 115.56 | loss  2.22 | ppl     9.19\n",
            "| epoch  23 |   300/ 1938 batches | ms/batch 115.49 | loss  2.23 | ppl     9.26\n",
            "| epoch  23 |   350/ 1938 batches | ms/batch 115.46 | loss  2.22 | ppl     9.20\n",
            "| epoch  23 |   400/ 1938 batches | ms/batch 115.25 | loss  2.22 | ppl     9.22\n",
            "| epoch  23 |   450/ 1938 batches | ms/batch 115.04 | loss  2.23 | ppl     9.34\n",
            "| epoch  23 |   500/ 1938 batches | ms/batch 114.78 | loss  2.22 | ppl     9.23\n",
            "| epoch  23 |   550/ 1938 batches | ms/batch 114.88 | loss  2.22 | ppl     9.23\n",
            "| epoch  23 |   600/ 1938 batches | ms/batch 114.74 | loss  2.23 | ppl     9.30\n",
            "| epoch  23 |   650/ 1938 batches | ms/batch 114.67 | loss  2.23 | ppl     9.29\n",
            "| epoch  23 |   700/ 1938 batches | ms/batch 114.29 | loss  2.23 | ppl     9.30\n",
            "| epoch  23 |   750/ 1938 batches | ms/batch 114.43 | loss  2.22 | ppl     9.22\n",
            "| epoch  23 |   800/ 1938 batches | ms/batch 114.34 | loss  2.22 | ppl     9.25\n",
            "| epoch  23 |   850/ 1938 batches | ms/batch 113.88 | loss  2.22 | ppl     9.20\n",
            "| epoch  23 |   900/ 1938 batches | ms/batch 114.51 | loss  2.23 | ppl     9.27\n",
            "| epoch  23 |   950/ 1938 batches | ms/batch 114.67 | loss  2.23 | ppl     9.29\n",
            "| epoch  23 |  1000/ 1938 batches | ms/batch 114.91 | loss  2.22 | ppl     9.21\n",
            "| epoch  23 |  1050/ 1938 batches | ms/batch 114.77 | loss  2.23 | ppl     9.27\n",
            "| epoch  23 |  1100/ 1938 batches | ms/batch 114.81 | loss  2.22 | ppl     9.24\n",
            "| epoch  23 |  1150/ 1938 batches | ms/batch 114.71 | loss  2.22 | ppl     9.19\n",
            "| epoch  23 |  1200/ 1938 batches | ms/batch 115.25 | loss  2.21 | ppl     9.16\n",
            "| epoch  23 |  1250/ 1938 batches | ms/batch 114.82 | loss  2.23 | ppl     9.29\n",
            "| epoch  23 |  1300/ 1938 batches | ms/batch 115.07 | loss  2.22 | ppl     9.22\n",
            "| epoch  23 |  1350/ 1938 batches | ms/batch 114.64 | loss  2.22 | ppl     9.25\n",
            "| epoch  23 |  1400/ 1938 batches | ms/batch 114.85 | loss  2.22 | ppl     9.25\n",
            "| epoch  23 |  1450/ 1938 batches | ms/batch 115.02 | loss  2.22 | ppl     9.20\n",
            "| epoch  23 |  1500/ 1938 batches | ms/batch 114.98 | loss  2.22 | ppl     9.25\n",
            "| epoch  23 |  1550/ 1938 batches | ms/batch 115.05 | loss  2.22 | ppl     9.24\n",
            "| epoch  23 |  1600/ 1938 batches | ms/batch 115.22 | loss  2.22 | ppl     9.20\n",
            "| epoch  23 |  1650/ 1938 batches | ms/batch 115.32 | loss  2.21 | ppl     9.16\n",
            "| epoch  23 |  1700/ 1938 batches | ms/batch 115.12 | loss  2.22 | ppl     9.20\n",
            "| epoch  23 |  1750/ 1938 batches | ms/batch 115.33 | loss  2.22 | ppl     9.19\n",
            "| epoch  23 |  1800/ 1938 batches | ms/batch 115.14 | loss  2.22 | ppl     9.22\n",
            "| epoch  23 |  1850/ 1938 batches | ms/batch 115.41 | loss  2.22 | ppl     9.25\n",
            "| epoch  23 |  1900/ 1938 batches | ms/batch 115.14 | loss  2.21 | ppl     9.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 311.92s | train loss  1.09 | train ppl     2.98 | valid loss  1.12 | valid ppl     3.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 23) to best_model_shakespeare_lstm.pt with Val PPL: 3.06 **\n",
            "| epoch  24 |    50/ 1938 batches | ms/batch 131.61 | loss  2.24 | ppl     9.43\n",
            "| epoch  24 |   100/ 1938 batches | ms/batch 114.84 | loss  2.21 | ppl     9.12\n",
            "| epoch  24 |   150/ 1938 batches | ms/batch 115.08 | loss  2.21 | ppl     9.09\n",
            "| epoch  24 |   200/ 1938 batches | ms/batch 114.78 | loss  2.21 | ppl     9.09\n",
            "| epoch  24 |   250/ 1938 batches | ms/batch 114.95 | loss  2.21 | ppl     9.10\n",
            "| epoch  24 |   300/ 1938 batches | ms/batch 115.17 | loss  2.21 | ppl     9.16\n",
            "| epoch  24 |   350/ 1938 batches | ms/batch 115.16 | loss  2.21 | ppl     9.08\n",
            "| epoch  24 |   400/ 1938 batches | ms/batch 115.26 | loss  2.21 | ppl     9.11\n",
            "| epoch  24 |   450/ 1938 batches | ms/batch 115.10 | loss  2.21 | ppl     9.15\n",
            "| epoch  24 |   500/ 1938 batches | ms/batch 115.28 | loss  2.22 | ppl     9.17\n",
            "| epoch  24 |   550/ 1938 batches | ms/batch 114.90 | loss  2.21 | ppl     9.14\n",
            "| epoch  24 |   600/ 1938 batches | ms/batch 115.30 | loss  2.21 | ppl     9.08\n",
            "| epoch  24 |   650/ 1938 batches | ms/batch 115.02 | loss  2.21 | ppl     9.09\n",
            "| epoch  24 |   700/ 1938 batches | ms/batch 115.37 | loss  2.21 | ppl     9.12\n",
            "| epoch  24 |   750/ 1938 batches | ms/batch 115.28 | loss  2.22 | ppl     9.16\n",
            "| epoch  24 |   800/ 1938 batches | ms/batch 115.31 | loss  2.21 | ppl     9.15\n",
            "| epoch  24 |   850/ 1938 batches | ms/batch 115.08 | loss  2.21 | ppl     9.10\n",
            "| epoch  24 |   900/ 1938 batches | ms/batch 115.22 | loss  2.21 | ppl     9.11\n",
            "| epoch  24 |   950/ 1938 batches | ms/batch 115.00 | loss  2.21 | ppl     9.09\n",
            "| epoch  24 |  1000/ 1938 batches | ms/batch 115.08 | loss  2.21 | ppl     9.10\n",
            "| epoch  24 |  1050/ 1938 batches | ms/batch 114.87 | loss  2.20 | ppl     9.07\n",
            "| epoch  24 |  1100/ 1938 batches | ms/batch 114.85 | loss  2.21 | ppl     9.12\n",
            "| epoch  24 |  1150/ 1938 batches | ms/batch 114.54 | loss  2.21 | ppl     9.14\n",
            "| epoch  24 |  1200/ 1938 batches | ms/batch 114.87 | loss  2.21 | ppl     9.11\n",
            "| epoch  24 |  1250/ 1938 batches | ms/batch 114.73 | loss  2.20 | ppl     9.04\n",
            "| epoch  24 |  1300/ 1938 batches | ms/batch 115.02 | loss  2.21 | ppl     9.07\n",
            "| epoch  24 |  1350/ 1938 batches | ms/batch 114.65 | loss  2.21 | ppl     9.09\n",
            "| epoch  24 |  1400/ 1938 batches | ms/batch 114.76 | loss  2.21 | ppl     9.09\n",
            "| epoch  24 |  1450/ 1938 batches | ms/batch 114.57 | loss  2.20 | ppl     9.06\n",
            "| epoch  24 |  1500/ 1938 batches | ms/batch 114.39 | loss  2.21 | ppl     9.15\n",
            "| epoch  24 |  1550/ 1938 batches | ms/batch 114.78 | loss  2.21 | ppl     9.09\n",
            "| epoch  24 |  1600/ 1938 batches | ms/batch 114.37 | loss  2.20 | ppl     9.07\n",
            "| epoch  24 |  1650/ 1938 batches | ms/batch 114.86 | loss  2.20 | ppl     9.05\n",
            "| epoch  24 |  1700/ 1938 batches | ms/batch 114.28 | loss  2.20 | ppl     9.05\n",
            "| epoch  24 |  1750/ 1938 batches | ms/batch 114.48 | loss  2.21 | ppl     9.10\n",
            "| epoch  24 |  1800/ 1938 batches | ms/batch 114.15 | loss  2.20 | ppl     9.07\n",
            "| epoch  24 |  1850/ 1938 batches | ms/batch 114.67 | loss  2.20 | ppl     9.05\n",
            "| epoch  24 |  1900/ 1938 batches | ms/batch 114.49 | loss  2.20 | ppl     9.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 311.55s | train loss  1.08 | train ppl     2.94 | valid loss  1.11 | valid ppl     3.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 24) to best_model_shakespeare_lstm.pt with Val PPL: 3.02 **\n",
            "| epoch  25 |    50/ 1938 batches | ms/batch 122.49 | loss  2.23 | ppl     9.27\n",
            "| epoch  25 |   100/ 1938 batches | ms/batch 114.88 | loss  2.19 | ppl     8.97\n",
            "| epoch  25 |   150/ 1938 batches | ms/batch 115.24 | loss  2.19 | ppl     8.95\n",
            "| epoch  25 |   200/ 1938 batches | ms/batch 115.19 | loss  2.20 | ppl     9.01\n",
            "| epoch  25 |   250/ 1938 batches | ms/batch 114.89 | loss  2.19 | ppl     8.95\n",
            "| epoch  25 |   300/ 1938 batches | ms/batch 115.27 | loss  2.20 | ppl     8.99\n",
            "| epoch  25 |   350/ 1938 batches | ms/batch 115.13 | loss  2.20 | ppl     8.99\n",
            "| epoch  25 |   400/ 1938 batches | ms/batch 115.39 | loss  2.20 | ppl     9.00\n",
            "| epoch  25 |   450/ 1938 batches | ms/batch 115.33 | loss  2.20 | ppl     9.04\n",
            "| epoch  25 |   500/ 1938 batches | ms/batch 115.31 | loss  2.19 | ppl     8.98\n",
            "| epoch  25 |   550/ 1938 batches | ms/batch 114.81 | loss  2.19 | ppl     8.96\n",
            "| epoch  25 |   600/ 1938 batches | ms/batch 115.01 | loss  2.19 | ppl     8.97\n",
            "| epoch  25 |   650/ 1938 batches | ms/batch 114.79 | loss  2.19 | ppl     8.97\n",
            "| epoch  25 |   700/ 1938 batches | ms/batch 115.01 | loss  2.19 | ppl     8.95\n",
            "| epoch  25 |   750/ 1938 batches | ms/batch 114.88 | loss  2.20 | ppl     8.99\n",
            "| epoch  25 |   800/ 1938 batches | ms/batch 115.12 | loss  2.19 | ppl     8.98\n",
            "| epoch  25 |   850/ 1938 batches | ms/batch 115.00 | loss  2.20 | ppl     9.02\n",
            "| epoch  25 |   900/ 1938 batches | ms/batch 115.02 | loss  2.19 | ppl     8.96\n",
            "| epoch  25 |   950/ 1938 batches | ms/batch 114.89 | loss  2.19 | ppl     8.95\n",
            "| epoch  25 |  1000/ 1938 batches | ms/batch 114.95 | loss  2.20 | ppl     9.01\n",
            "| epoch  25 |  1050/ 1938 batches | ms/batch 114.89 | loss  2.19 | ppl     8.97\n",
            "| epoch  25 |  1100/ 1938 batches | ms/batch 114.91 | loss  2.20 | ppl     9.01\n",
            "| epoch  25 |  1150/ 1938 batches | ms/batch 114.40 | loss  2.19 | ppl     8.96\n",
            "| epoch  25 |  1200/ 1938 batches | ms/batch 114.66 | loss  2.20 | ppl     8.99\n",
            "| epoch  25 |  1250/ 1938 batches | ms/batch 114.67 | loss  2.20 | ppl     9.01\n",
            "| epoch  25 |  1300/ 1938 batches | ms/batch 114.32 | loss  2.19 | ppl     8.95\n",
            "| epoch  25 |  1350/ 1938 batches | ms/batch 114.48 | loss  2.19 | ppl     8.95\n",
            "| epoch  25 |  1400/ 1938 batches | ms/batch 114.40 | loss  2.19 | ppl     8.98\n",
            "| epoch  25 |  1450/ 1938 batches | ms/batch 114.53 | loss  2.19 | ppl     8.92\n",
            "| epoch  25 |  1500/ 1938 batches | ms/batch 114.56 | loss  2.19 | ppl     8.97\n",
            "| epoch  25 |  1550/ 1938 batches | ms/batch 114.65 | loss  2.19 | ppl     8.97\n",
            "| epoch  25 |  1600/ 1938 batches | ms/batch 114.49 | loss  2.19 | ppl     8.98\n",
            "| epoch  25 |  1650/ 1938 batches | ms/batch 114.56 | loss  2.19 | ppl     8.94\n",
            "| epoch  25 |  1700/ 1938 batches | ms/batch 114.19 | loss  2.19 | ppl     8.97\n",
            "| epoch  25 |  1750/ 1938 batches | ms/batch 114.50 | loss  2.18 | ppl     8.86\n",
            "| epoch  25 |  1800/ 1938 batches | ms/batch 114.04 | loss  2.19 | ppl     8.94\n",
            "| epoch  25 |  1850/ 1938 batches | ms/batch 114.55 | loss  2.19 | ppl     8.93\n",
            "| epoch  25 |  1900/ 1938 batches | ms/batch 114.31 | loss  2.19 | ppl     8.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 310.97s | train loss  1.06 | train ppl     2.89 | valid loss  1.09 | valid ppl     2.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "** Saved best model (Epoch 25) to best_model_shakespeare_lstm.pt with Val PPL: 2.97 **\n",
            "\n",
            "Loading best model from best_model_shakespeare_lstm.pt for final steps.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Evaluation on Validation Set ---\n",
            "Loading best model from best_model_shakespeare_lstm.pt for final evaluation.\n",
            "Final Perplexity on Validation Data: 2.97 (Loss: 1.09)\n",
            "\n",
            "--- Text Generation Samples ---\n",
            "Prime Text: 'JULIET:'\n",
            "\n",
            "Generating 2 sample(s) with Temperature Sampling (Temp: 0.8):\n",
            "\n",
            "--- Sample 1 ---\n",
            "JULIET : Take me the bastard , and let me be king : Be not a man , it , I should be a child . MERCUTIO : To have a dispatch , sir , to it with me . DUKE VINCENTIO : She hath release d him , Isabel , to your brother ; To , and so are a man to thee . But do you not request the most re ach ing by this isle , to make them a king , And last to her that ne ' er was married for . I know not what to make me of your kindred , and pluck him with ing heart , as thou art , to make me a villain : ing my good lord and me ; I am not for them : and the next morn betimes , And that be p ous with the oak s blood That now this bear her day ? O , then she ' s gone ; I ' ll wear the first ow garland of the run . He will not call her hand , nor by a love ! TRANIO : O slow , O , what love , sir , if you be rememb ered , cracking the stones of his sweet demands ! SICINIUS : He ' s sentenced ; but this ' s soldiers is The common way , sir . The drum , for all men have the master ' s life . First Citizen : But here comes all . LEONTES : That ' s not good ; but rather must I call it The more of all that is it in . NORTHUMBERLAND : But ' twas such grief to - morrow , But for my heart ' s dear love you have not too . KING RICHARD III : Good Norfolk , hie thee to thy charge ; U se careful watch , choose trusty sent ine ls . NORFOLK : My lord , my lord , I ' ll tell thee what he is ; And you must not be married in this world . CAPULET : I pray , my lord , for this is some old news . GLOUCESTER : Come , come , you say I should . DUKE VINCENTIO : Give me his glass ; and here ' s the king to give him all . DUKE VINCENTIO : It did your husband , if a noble prince , And hither , to the worse . CORIOLANUS : The god i ' the low est hell fold ation , and the rest , And touch ' d that which is nothing by a man , This is some part with me . Come , let us go . Now , by Saint Paul I swear , thy soul shall prove As benef its to me , and I ' ll A cup of mine man . PAULINA : I think ' tis now , I '\n",
            "----------------------------------------\n",
            "\n",
            "--- Sample 2 ---\n",
            "JULIET : Who is a cause ? O , where I will have in this vi per as I heard , Even to the crown ' s . But , as I am a man ' s - gracious eye , I say . Nurse : It is . What sadness lengthen s Romeo ' s hours ? LADY CAPULET : O that ! how , that ' s a name of all . PETRUCHIO : Why , how now , Claudio ! whence are this restraint ? PETRUCHIO : For Mantua , sir , to you . PROSPERO : For that the king , which would not suffer us , O pin - one day , the king ' s in all our children ! As any , thou art deceived : I have but lief die by my name , but I am not in mine . ROMEO : Not I , believe him . GLOUCESTER : Then shall I be , that I may longest keep The part of that it is that shall prove more than a man ' s . Will you go to a little day by time ! PETRUCHIO : I know you well . PETRUCHIO : Now , for the time , I hear him well -- and I did not bid him speak . Come , if you be a king in the common weal , he is not too much . Second Murderer : How he hath beat the king with child ' s head ,-- It is not , by that word ' s , and afterward s ' I thank you then ;' And yet remain and she ' s well - found ed , to be revenged . HENRY BOLINGBROKE : Good aunt , stand up . DUCHESS OF YORK : O , let me breathe ! KING RICHARD II : Aumerle , shall give this counsel in our life ; For then he bid me stay : and die for me , I ' ll help you to my brother and my wife . KATHARINA : That thou wert true ; and I do love my wit . Who ' s there ? BALTHASAR : I am Christopher of him , and take it on ; O , how I am a gentleman of your feast es . First Huntsman : The dignity , you hath but the king ' s , And give him us and make us away . The day will not be something of a blow . ROMEO : I will be mild , and not stand a man ? Then , as I were a king , I ' ll win a meet : ' Hic ibat Si mois ,' I know you not , ' hic est Si geia tellus ,' I trust you not . HORTENSIO : What , in the wheels of Caesar ? I do not think it . GRUMIO : God ' s Romeo wrong ! And ,\n",
            "----------------------------------------\n",
            "\n",
            "Beam search disabled (beam_width=0).\n"
          ]
        }
      ],
      "source": [
        "#@title 1. Installations\n",
        "# Install necessary libraries\n",
        "!pip install torch numpy tokenizers nltk scikit-learn matplotlib -q\n",
        "\n",
        "print(\"Libraries installed.\")\n",
        "\n",
        "#@title 2. NLTK Data (Optional, needed for --stemming)\n",
        "import nltk\n",
        "\n",
        "try:\n",
        "    print(\"Checking/downloading NLTK data (punkt, wordnet, omw-1.4)...\")\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "    nltk.download('omw-1.4', quiet=True)\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "    nltk.data.find('corpora/omw-1.4')\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    print(\"NLTK data seems available.\")\n",
        "    nltk_available = True\n",
        "except LookupError:\n",
        "    print(\"---------------------------------------------------------\")\n",
        "    print(\"NLTK data download might be needed or failed previously.\")\n",
        "    print(\"If stemming fails later, run the following in a code cell:\")\n",
        "    print(\"import nltk; nltk.download('punkt'); nltk.download('wordnet'); nltk.download('omw-1.4')\")\n",
        "    print(\"---------------------------------------------------------\")\n",
        "    nltk_available = False # Assume failure for now if lookup fails\n",
        "except Exception as e:\n",
        "     print(f\"An error occurred during NLTK check/download: {e}\")\n",
        "     nltk_available = False\n",
        "\n",
        "#@title 3. Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import heapq\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# NLTK for optional stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "# (nltk_available checked later)\n",
        "\n",
        "# Tokenizers library\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE, WordPiece\n",
        "from tokenizers.trainers import BpeTrainer, WordPieceTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing # Useful for adding special tokens\n",
        "\n",
        "# For file uploading in Colab\n",
        "from google.colab import files\n",
        "\n",
        "#@title 4. Configuration (Modify parameters here)\n",
        "\n",
        "class Config:\n",
        "    # --- Data and Tokenizer ---\n",
        "    data_file = 'shakespeare.txt' # Will be uploaded to this name in Colab\n",
        "    tokenizer_type = 'bpe'         # 'bpe' or 'wordpiece'\n",
        "    vocab_size = 8000              # Target vocabulary size\n",
        "    # Tokenizer path uses type and vocab size automatically\n",
        "    tokenizer_path_template = 'shakespeare_tokenizer_{type}_{vs}{stem_suffix}.json'\n",
        "    stemming = False               # Set to True to enable stemming (requires NLTK download)\n",
        "\n",
        "    # --- Model Architecture ---\n",
        "    model = 'lstm'                 # 'rnn', 'lstm', or 'gru'\n",
        "    embedding_dim = 256\n",
        "    hidden_dim = 512\n",
        "    num_layers = 2\n",
        "    bidirectional = False          # Set to True for bidirectional\n",
        "    dropout = 0.5\n",
        "\n",
        "    # --- Training ---\n",
        "    seq_len = 60                   # Sequence length in tokens\n",
        "    batch_size = 128\n",
        "    epochs = 25                    # Adjust as needed (can take time)\n",
        "    lr = 0.002\n",
        "    optimizer = 'adam'             # 'adam' or 'rmsprop'\n",
        "    clip = 1.0                     # Gradient clipping value\n",
        "    seed = 42\n",
        "    log_interval = 50              # Batches between logs\n",
        "    validation_split = 0.1         # 10% for validation\n",
        "\n",
        "    # --- Model Saving/Loading ---\n",
        "    # Save path uses model type and bidir status automatically\n",
        "    save_path_template = 'best_model_shakespeare_{model_name}.pt'\n",
        "    load_path = None               # Set to a path (e.g., 'best_model_shakespeare_lstm.pt') to load and skip training\n",
        "\n",
        "    # --- Generation ---\n",
        "    generate_len = 500             # Number of tokens to generate\n",
        "    temperature = 0.8              # Temperature for sampling\n",
        "    beam_width = 0                 # Beam width for beam search (0 disables beam search)\n",
        "    num_samples = 2                # Number of samples to generate for each method\n",
        "    prime_text = 'JULIET:'         # Seed text\n",
        "\n",
        "    # --- Environment ---\n",
        "    disable_cuda = False           # Set to True to force CPU\n",
        "\n",
        "\n",
        "# Instantiate config\n",
        "config = Config()\n",
        "\n",
        "# Disable stemming if NLTK isn't ready\n",
        "if config.stemming and not nltk_available:\n",
        "    print(\"Warning: NLTK not available or setup failed. Disabling stemming in config.\")\n",
        "    config.stemming = False\n",
        "\n",
        "# Format tokenizer and save paths dynamically\n",
        "stem_suffix = '_stemmed' if config.stemming else ''\n",
        "config.tokenizer_path = config.tokenizer_path_template.format(\n",
        "    type=config.tokenizer_type, vs=config.vocab_size, stem_suffix=stem_suffix\n",
        ")\n",
        "model_name = f\"{config.model}{'_bi' if config.bidirectional else ''}\"\n",
        "config.save_path = config.save_path_template.format(model_name=model_name)\n",
        "\n",
        "# --- 5. Setup Device and Seed ---\n",
        "torch.manual_seed(config.seed)\n",
        "np.random.seed(config.seed)\n",
        "random.seed(config.seed)\n",
        "use_cuda = not config.disable_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Tokenizer will be saved/loaded from: {config.tokenizer_path}\")\n",
        "print(f\"Best model will be saved to: {config.save_path}\")\n",
        "\n",
        "\n",
        "#@title 6. Helper Function: Stemming\n",
        "def apply_stemming(text):\n",
        "    \"\"\"Applies Porter stemming to the text.\"\"\"\n",
        "    if not nltk_available:\n",
        "        print(\"Skipping stemming as NLTK is not available.\")\n",
        "        return text\n",
        "    print(\"Applying stemming...\")\n",
        "    stemmer = PorterStemmer()\n",
        "    # Tokenize into words first for stemming\n",
        "    try:\n",
        "        words = nltk.word_tokenize(text)\n",
        "        stemmed_words = [stemmer.stem(word) for word in words]\n",
        "        print(\"Stemming complete.\")\n",
        "        return ' '.join(stemmed_words)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during NLTK word tokenization/stemming: {e}\")\n",
        "        print(\"Proceeding without stemming.\")\n",
        "        return text # Return original text if error\n",
        "\n",
        "#@title 7. Tokenizer Function\n",
        "def train_or_load_tokenizer(data_content, tokenizer_path, vocab_size, tokenizer_type='bpe', apply_stem=False):\n",
        "    \"\"\"Trains or loads a BPE/WordPiece tokenizer from text content.\"\"\"\n",
        "\n",
        "    # Check existence based on potentially modified path if stemming\n",
        "    final_tokenizer_path = tokenizer_path\n",
        "    if apply_stem:\n",
        "         base, ext = os.path.splitext(tokenizer_path)\n",
        "         # Ensure template wasn't already applied\n",
        "         if '{type}' not in base and '{vs}' not in base:\n",
        "              final_tokenizer_path = f\"{base.replace('_stemmed','')}_stemmed{ext}\"\n",
        "         else: # Apply template now if it wasn't before\n",
        "             stem_suffix = '_stemmed'\n",
        "             final_tokenizer_path = config.tokenizer_path_template.format(\n",
        "                type=config.tokenizer_type, vs=config.vocab_size, stem_suffix=stem_suffix\n",
        "             )\n",
        "         print(f\"Checking for stemmed tokenizer at: {final_tokenizer_path}\")\n",
        "\n",
        "\n",
        "    if os.path.exists(final_tokenizer_path):\n",
        "        print(f\"Loading existing tokenizer from {final_tokenizer_path}\")\n",
        "        try:\n",
        "            tokenizer = Tokenizer.from_file(final_tokenizer_path)\n",
        "            if tokenizer.get_vocab_size() < vocab_size: # Allow larger existing vocab\n",
        "                 print(f\"Warning: Loaded tokenizer vocab size ({tokenizer.get_vocab_size()}) is smaller than requested ({vocab_size}).\")\n",
        "                 # Decide if retraining is necessary - for simplicity, let's use the loaded one\n",
        "            return tokenizer\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading tokenizer: {e}. Will attempt to retrain.\")\n",
        "\n",
        "    print(f\"Training new {tokenizer_type.upper()} tokenizer{' with stemming' if apply_stem else ''}...\")\n",
        "    text_to_train = apply_stemming(data_content) if apply_stem else data_content\n",
        "\n",
        "    if not text_to_train:\n",
        "         raise ValueError(\"No text content provided for tokenizer training.\")\n",
        "\n",
        "    special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] # Common special tokens\n",
        "\n",
        "    if tokenizer_type == 'bpe':\n",
        "        tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "        trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=special_tokens)\n",
        "    elif tokenizer_type == 'wordpiece':\n",
        "        tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "        trainer = WordPieceTrainer(vocab_size=vocab_size, special_tokens=special_tokens)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported tokenizer type. Choose 'bpe' or 'wordpiece'.\")\n",
        "\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "    # Train using an iterator\n",
        "    def text_iterator():\n",
        "        # Process in chunks if memory is a concern, here simple yield\n",
        "        yield text_to_train\n",
        "\n",
        "    tokenizer.train_from_iterator(text_iterator(), trainer, length=len(text_to_train))\n",
        "\n",
        "    # Set post-processor (optional, adds CLS/SEP for some tasks, not strictly needed for generation)\n",
        "    # tokenizer.post_processor = TemplateProcessing(\n",
        "    #     single=\"[CLS] $A [SEP]\",\n",
        "    #     pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    #     special_tokens=[\n",
        "    #         (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
        "    #         (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
        "    #     ],\n",
        "    # )\n",
        "\n",
        "\n",
        "    # Save the tokenizer\n",
        "    try:\n",
        "        # Use the final_tokenizer_path determined earlier\n",
        "        tokenizer.save(final_tokenizer_path)\n",
        "        print(f\"Tokenizer trained and saved to {final_tokenizer_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving tokenizer to {final_tokenizer_path}: {e}\")\n",
        "\n",
        "    # Ensure PAD token exists after training/loading\n",
        "    if tokenizer.token_to_id(\"[PAD]\") is None:\n",
        "         tokenizer.add_special_tokens([\"[PAD]\"])\n",
        "         print(\"Added [PAD] token.\")\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "#@title 8. Upload Dataset\n",
        "# Upload the shakespeare.txt file\n",
        "print(f\"Please upload the dataset file ('{config.data_file}' or specify different name in Config)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Verify upload and read content\n",
        "if config.data_file not in uploaded:\n",
        "    if len(uploaded) == 1:\n",
        "        # If only one file was uploaded, assume it's the correct one\n",
        "        uploaded_filename = list(uploaded.keys())[0]\n",
        "        print(f\"Assuming uploaded file '{uploaded_filename}' is the dataset.\")\n",
        "        config.data_file = uploaded_filename # Update config if name differs\n",
        "        raw_text = uploaded[uploaded_filename].decode('utf-8')\n",
        "    else:\n",
        "        raise ValueError(f\"File '{config.data_file}' not found in upload. Please upload the correct file.\")\n",
        "else:\n",
        "    raw_text = uploaded[config.data_file].decode('utf-8')\n",
        "\n",
        "print(f\"Successfully loaded {len(raw_text)} characters from {config.data_file}\")\n",
        "\n",
        "# --- Train or Load Tokenizer ---\n",
        "# Pass the actual text content now\n",
        "try:\n",
        "    tokenizer = train_or_load_tokenizer(raw_text, config.tokenizer_path, config.vocab_size, config.tokenizer_type, config.stemming)\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- Error during tokenizer step ---\")\n",
        "    print(e)\n",
        "    print(\"Please check file content, permissions, or NLTK setup if using stemming.\")\n",
        "    # Stop execution if tokenizer fails critically\n",
        "    raise\n",
        "\n",
        "\n",
        "#@title 9. Dataset Class (Subword)\n",
        "class SubwordTextDataset(Dataset):\n",
        "    def __init__(self, text, seq_len, tokenizer, apply_stem=False):\n",
        "        self.seq_len = seq_len\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab_size = tokenizer.get_vocab_size()\n",
        "        print(\"Processing text for dataset...\")\n",
        "        processed_text = apply_stemming(text) if apply_stem else text\n",
        "\n",
        "        print(\"Encoding text with tokenizer...\")\n",
        "        # Encode the whole text - okay for Shakespeare size\n",
        "        self.encoded_ids = tokenizer.encode(processed_text).ids\n",
        "        print(\"Encoding complete.\")\n",
        "\n",
        "        self.sequences = []\n",
        "        self.targets = []\n",
        "        if len(self.encoded_ids) <= seq_len:\n",
        "             raise ValueError(f\"Processed text is too short ({len(self.encoded_ids)} tokens) for sequence length {seq_len}\")\n",
        "\n",
        "        print(\"Creating sequences...\")\n",
        "        for i in range(0, len(self.encoded_ids) - seq_len):\n",
        "            # Input sequence: tokens from i to i+seq_len-1\n",
        "            seq_in = self.encoded_ids[i : i + seq_len]\n",
        "            # Target sequence: tokens from i+1 to i+seq_len\n",
        "            seq_out = self.encoded_ids[i + 1 : i + seq_len + 1]\n",
        "            self.sequences.append(seq_in)\n",
        "            self.targets.append(seq_out)\n",
        "        print(\"Sequence creation complete.\")\n",
        "\n",
        "        print(f\"\\nTokenizer Vocabulary size: {self.vocab_size}\")\n",
        "        print(f\"Total tokens in processed text: {len(self.encoded_ids)}\")\n",
        "        print(f\"Total sequences created: {len(self.sequences)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (torch.tensor(self.sequences[index], dtype=torch.long),\n",
        "                torch.tensor(self.targets[index], dtype=torch.long))\n",
        "\n",
        "#@title 10. Create Dataset and Dataloaders\n",
        "# Create dataset\n",
        "dataset = SubwordTextDataset(raw_text, config.seq_len, tokenizer, config.stemming)\n",
        "\n",
        "# Create Train/Validation Split\n",
        "dataset_size = len(dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(config.validation_split * dataset_size))\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Handle case where split might be 0\n",
        "if split == 0:\n",
        "     print(\"Warning: Validation split is 0, using full dataset for training and evaluation.\")\n",
        "     train_indices = indices\n",
        "     val_indices = indices # Use all data if no validation split\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "validation_sampler = SubsetRandomSampler(val_indices) # Use SubsetRandomSampler for validation too\n",
        "\n",
        "train_dataloader = DataLoader(dataset, batch_size=config.batch_size, sampler=train_sampler, drop_last=True)\n",
        "# Important: Use drop_last=True for validation too, to ensure consistent batch sizes for hidden state initialization in evaluate func.\n",
        "# Alternatively, modify evaluate to handle variable last batch size. Let's keep drop_last=True for simplicity here.\n",
        "validation_dataloader = DataLoader(dataset, batch_size=config.batch_size, sampler=validation_sampler, drop_last=True)\n",
        "\n",
        "print(f\"Training set size: {len(train_indices)}\")\n",
        "print(f\"Validation set size: {len(val_indices)}\")\n",
        "print(f\"Number of training batches: {len(train_dataloader)}\")\n",
        "print(f\"Number of validation batches: {len(validation_dataloader)}\")\n",
        "\n",
        "\n",
        "#@title 11. Model Definition\n",
        "class RNNModel(nn.Module):\n",
        "    \"\"\" Generic RNN model adaptable for RNN, LSTM, or GRU units. \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers,\n",
        "                 dropout=0.5, model_type='lstm', bidirectional=False):\n",
        "        super().__init__()\n",
        "        self.model_type = model_type.lower()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        rnn_layer_args = {\n",
        "            'input_size': embedding_dim,\n",
        "            'hidden_size': hidden_dim,\n",
        "            'num_layers': num_layers,\n",
        "            'dropout': dropout if num_layers > 1 else 0,\n",
        "            'batch_first': True,\n",
        "            'bidirectional': bidirectional\n",
        "        }\n",
        "\n",
        "        if self.model_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(**rnn_layer_args)\n",
        "        elif self.model_type == 'gru':\n",
        "            self.rnn = nn.GRU(**rnn_layer_args)\n",
        "        elif self.model_type == 'rnn':\n",
        "            self.rnn = nn.RNN(rnn_layer_args['input_size'], rnn_layer_args['hidden_size'],\n",
        "                             rnn_layer_args['num_layers'], batch_first=True,\n",
        "                             bidirectional=bidirectional,\n",
        "                             dropout=rnn_layer_args['dropout'], nonlinearity='tanh') # Added nonlinearity\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported RNN type. Choose 'rnn', 'lstm', or 'gru'.\")\n",
        "\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * self.num_directions, vocab_size)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, hidden):\n",
        "        # src shape: [batch_size, seq_len]\n",
        "        embedded = self.dropout_layer(self.embedding(src))\n",
        "        # embedded shape: [batch_size, seq_len, embedding_dim]\n",
        "\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        # output shape: [batch_size, seq_len, hidden_dim * num_directions]\n",
        "\n",
        "        output = self.dropout_layer(output)\n",
        "\n",
        "        # decoded shape: [batch_size * seq_len, vocab_size]\n",
        "        decoded = self.fc(output.reshape(output.size(0) * output.size(1), output.size(2)))\n",
        "\n",
        "        # return shape: [batch_size, seq_len, vocab_size], hidden_state\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        layers_x_directions = self.num_layers * self.num_directions\n",
        "        if self.model_type == 'lstm':\n",
        "            # LSTM hidden state: (h_n, c_n)\n",
        "            h_0 = weight.new(layers_x_directions, batch_size, self.hidden_dim).zero_().to(device)\n",
        "            c_0 = weight.new(layers_x_directions, batch_size, self.hidden_dim).zero_().to(device)\n",
        "            return (h_0, c_0)\n",
        "        else: # GRU or RNN hidden state: h_n\n",
        "            return weight.new(layers_x_directions, batch_size, self.hidden_dim).zero_().to(device)\n",
        "\n",
        "#@title 12. Instantiate Model, Loss, Optimizer\n",
        "# Instantiate the model\n",
        "model = RNNModel(\n",
        "    tokenizer.get_vocab_size(),\n",
        "    config.embedding_dim,\n",
        "    config.hidden_dim,\n",
        "    config.num_layers,\n",
        "    config.dropout,\n",
        "    config.model,\n",
        "    config.bidirectional\n",
        ").to(device)\n",
        "\n",
        "print(\"\\n--- Model Architecture ---\")\n",
        "print(model)\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters: {total_params:,}\")\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "if config.optimizer == 'adam':\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "elif config.optimizer == 'rmsprop':\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=config.lr)\n",
        "else:\n",
        "    raise ValueError(\"Unsupported optimizer type.\")\n",
        "\n",
        "# Optional: Learning Rate Scheduler\n",
        "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8) # Example: Decay LR every 5 epochs\n",
        "\n",
        "#@title 13. Training and Evaluation Functions\n",
        "# (Using train and evaluate functions defined previously - no code needed here, just run if modified)\n",
        "\n",
        "def train(model, dataloader, epoch, criterion, optimizer, clip, vocab_size):\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden(config.batch_size) # Use config batch size\n",
        "\n",
        "    num_batches = len(dataloader)\n",
        "    for batch_idx, (data, targets) in enumerate(dataloader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        # Starting each batch with a detached hidden state is crucial for truncated BPTT\n",
        "        if isinstance(hidden, tuple): # LSTM\n",
        "            hidden = tuple([h.detach() for h in hidden])\n",
        "        else: # GRU/RNN\n",
        "            hidden = hidden.detach()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output, hidden = model(data, hidden) # hidden is updated in place\n",
        "        # Reshape for loss calculation:\n",
        "        # Output: [batch_size, seq_len, vocab_size] -> [batch_size * seq_len, vocab_size]\n",
        "        # Target: [batch_size, seq_len] -> [batch_size * seq_len]\n",
        "        loss = criterion(output.view(-1, vocab_size), targets.reshape(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % config.log_interval == 0 and batch_idx > 0:\n",
        "            cur_loss = total_loss / config.log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            ppl = math.exp(cur_loss) if cur_loss > 0 else float('inf')\n",
        "            print(f'| epoch {epoch:3d} | {batch_idx:5d}/{num_batches:5d} batches | '\n",
        "                  # f'lr {optimizer.param_groups[0][\"lr\"]:.2e} | ' # Uncomment if using scheduler\n",
        "                  f'ms/batch {elapsed * 1000 / config.log_interval:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0.\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, vocab_size):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    total_tokens = 0\n",
        "\n",
        "    # Use the dataloader's actual batch size if different from config (esp. for validation)\n",
        "    eval_batch_size = dataloader.batch_size\n",
        "    hidden = model.init_hidden(eval_batch_size) # Initialize hidden state for eval batch size\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, targets in dataloader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            # Detach hidden state before processing batch\n",
        "            if isinstance(hidden, tuple): # LSTM\n",
        "                 hidden = tuple([h.detach() for h in hidden])\n",
        "            else: # GRU/RNN\n",
        "                 hidden = hidden.detach()\n",
        "\n",
        "             # Handle potential last batch size mismatch if drop_last=False\n",
        "            if data.size(0) != eval_batch_size:\n",
        "                 hidden = model.init_hidden(data.size(0)) # Re-init if batch size changes\n",
        "                 eval_batch_size = data.size(0) # Update current batch size\n",
        "\n",
        "\n",
        "            output, hidden = model(data, hidden)\n",
        "            output_flat = output.view(-1, vocab_size)\n",
        "            targets_flat = targets.reshape(-1)\n",
        "            loss = criterion(output_flat, targets_flat)\n",
        "            total_loss += loss.item() * targets_flat.size(0)\n",
        "            total_tokens += targets_flat.size(0)\n",
        "\n",
        "    if total_tokens == 0: return float('inf'), float('inf')\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = math.exp(avg_loss) if avg_loss > 0 else float('inf')\n",
        "    return avg_loss, perplexity\n",
        "\n",
        "#@title 14. Generation Functions (Temperature and Beam Search)\n",
        "# (Using generate_text_temperature and generate_text_beam_search defined previously)\n",
        "\n",
        "# Temperature Sampling\n",
        "def generate_text_temperature(model, start_string, length, temperature, tokenizer, device):\n",
        "    model.eval()\n",
        "    generated_ids = []\n",
        "\n",
        "    prime_ids = tokenizer.encode(start_string).ids\n",
        "    if not prime_ids:\n",
        "        print(\"Warning: Prime text encoded to empty sequence. Using space.\")\n",
        "        start_token_id = tokenizer.token_to_id(\" \") or 0\n",
        "        prime_ids = [start_token_id]\n",
        "\n",
        "    current_sequence = torch.tensor([prime_ids], dtype=torch.long).to(device)\n",
        "    hidden = model.init_hidden(1) # Batch size 1\n",
        "\n",
        "    # Warm up hidden state\n",
        "    with torch.no_grad():\n",
        "        init_input = current_sequence[:, -config.seq_len:] # Use config seq_len for warmup consistency\n",
        "        output_init, hidden = model(init_input, hidden)\n",
        "        # Use the last token of the prime sequence as the first input for generation\n",
        "        input_token_id = current_sequence[:, -1].unsqueeze(0) # Shape: [1, 1]\n",
        "\n",
        "    generated_ids.extend(prime_ids)\n",
        "\n",
        "    for _ in range(length):\n",
        "        with torch.no_grad():\n",
        "            if isinstance(hidden, tuple): # LSTM\n",
        "                hidden = tuple([h.detach() for h in hidden])\n",
        "            else: # GRU/RNN\n",
        "                hidden = hidden.detach()\n",
        "\n",
        "            output, hidden = model(input_token_id, hidden)\n",
        "            # Output shape is [1, 1, vocab_size], take the last time step's output\n",
        "            output_last_step = output[:, -1, :] # Shape: [1, vocab_size]\n",
        "            output_dist = output_last_step.squeeze().div(temperature).exp() # Shape: [vocab_size]\n",
        "\n",
        "            # Handle potential numerical instability\n",
        "            if torch.isinf(output_dist).any() or torch.isnan(output_dist).any():\n",
        "                print(\"Warning: Numerical instability in output distribution. Sampling uniformly.\")\n",
        "                top_token_id = random.randrange(tokenizer.get_vocab_size())\n",
        "            else:\n",
        "                top_token_id = torch.multinomial(output_dist, 1)[0].item()\n",
        "\n",
        "\n",
        "            generated_ids.append(top_token_id)\n",
        "            input_token_id = torch.tensor([[top_token_id]], dtype=torch.long).to(device)\n",
        "\n",
        "    # Decode and clean up\n",
        "    text = tokenizer.decode(generated_ids)\n",
        "    text = text.replace(\"[UNK]\", \"?\")\n",
        "    text = text.replace(\" ##\", \"\").replace(\"##\", \"\") # WordPiece cleanup\n",
        "    text = text.replace(\"Ġ\", \" \") # BPE space cleanup\n",
        "    return text\n",
        "\n",
        "# Beam Search Generation\n",
        "def generate_text_beam_search(model, start_string, length, beam_width, tokenizer, device):\n",
        "    model.eval()\n",
        "\n",
        "    prime_ids = tokenizer.encode(start_string).ids\n",
        "    if not prime_ids:\n",
        "        print(\"Warning: Prime text encoded to empty sequence for beam search. Using space.\")\n",
        "        start_token_id = tokenizer.token_to_id(\" \") or 0\n",
        "        prime_ids = [start_token_id]\n",
        "\n",
        "    start_sequence_tensor = torch.tensor([prime_ids], dtype=torch.long).to(device)\n",
        "\n",
        "    # Initialize hidden state based on prime text\n",
        "    hidden = model.init_hidden(1) # Batch size 1\n",
        "    with torch.no_grad():\n",
        "        init_input = start_sequence_tensor[:, -config.seq_len:]\n",
        "        _, hidden = model(init_input, hidden)\n",
        "\n",
        "    # Beam search initialization\n",
        "    # Each beam: (log_probability, sequence_ids_list, hidden_state)\n",
        "    # Use negative log probability, heapq finds minimum\n",
        "    beams = [(-0.0, prime_ids, hidden)]\n",
        "\n",
        "    for _ in range(length):\n",
        "        new_beams_candidates = []\n",
        "        for log_prob, seq_ids, current_hidden in beams:\n",
        "            # Detach hidden state\n",
        "            if isinstance(current_hidden, tuple): # LSTM\n",
        "                current_hidden_detached = tuple([h.detach() for h in current_hidden])\n",
        "            else: # GRU/RNN\n",
        "                current_hidden_detached = current_hidden.detach()\n",
        "\n",
        "            # Get the last token ID as input\n",
        "            last_token_id = seq_ids[-1]\n",
        "            input_token = torch.tensor([[last_token_id]], dtype=torch.long).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output, next_hidden = model(input_token, current_hidden_detached)\n",
        "\n",
        "            # Get log probabilities for the next token\n",
        "            log_probs = F.log_softmax(output.squeeze(), dim=0) # Shape [vocab_size]\n",
        "\n",
        "            # Get top k log probabilities and indices\n",
        "            top_log_probs, top_indices = torch.topk(log_probs, beam_width)\n",
        "\n",
        "            # Expand the current beam\n",
        "            for i in range(beam_width):\n",
        "                next_token_id = top_indices[i].item()\n",
        "                # Handle padding or end tokens if necessary\n",
        "                # if next_token_id == tokenizer.token_to_id(\"[PAD]\"): continue\n",
        "                new_log_prob = log_prob + top_log_probs[i].item() # Add log probs\n",
        "                new_seq_ids = seq_ids + [next_token_id]\n",
        "                # Store as (neg_log_prob, seq, hidden) for min-heap\n",
        "                new_beams_candidates.append((-new_log_prob, new_seq_ids, next_hidden))\n",
        "\n",
        "        # Prune beams efficiently using heapq\n",
        "        # Keep track of the best beam_width candidates based on log probability\n",
        "        if len(new_beams_candidates) > beam_width:\n",
        "            best_new_beams = heapq.nsmallest(beam_width, new_beams_candidates, key=lambda x: x[0])\n",
        "        else:\n",
        "            best_new_beams = new_beams_candidates\n",
        "\n",
        "        # Convert back to (log_prob, seq, hidden) format\n",
        "        beams = [(-neg_log_prob, seq, hid) for neg_log_prob, seq, hid in best_new_beams]\n",
        "\n",
        "        # Early stopping condition (e.g., if all beams end with EOS token) could be added here\n",
        "\n",
        "    # Select the beam with the highest log probability (lowest negative log prob)\n",
        "    best_log_prob, best_seq_ids, _ = beams[0]\n",
        "\n",
        "    # Decode and clean up\n",
        "    text = tokenizer.decode(best_seq_ids)\n",
        "    text = text.replace(\"[UNK]\", \"?\")\n",
        "    text = text.replace(\" ##\", \"\").replace(\"##\", \"\")\n",
        "    text = text.replace(\"Ġ\", \" \")\n",
        "    return text\n",
        "\n",
        "\n",
        "#@title 15. Training Loop Execution\n",
        "best_val_perplexity = float('inf')\n",
        "train_losses, val_losses = [], []\n",
        "train_perplexities, val_perplexities = [], []\n",
        "start_epoch = 1\n",
        "epochs_to_run = config.epochs\n",
        "\n",
        "if config.load_path and os.path.exists(config.load_path):\n",
        "    try:\n",
        "        print(f\"Loading pre-trained model from {config.load_path}...\")\n",
        "        model.load_state_dict(torch.load(config.load_path, map_location=device))\n",
        "        print(\"Model loaded successfully. Skipping training.\")\n",
        "        epochs_to_run = 0 # Do not train if loading\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load model from {config.load_path}. Error: {e}. Training from scratch.\")\n",
        "        config.load_path = None # Ensure training happens if loading fails\n",
        "else:\n",
        "    print(\"No pre-trained model specified or found. Training from scratch.\")\n",
        "\n",
        "\n",
        "if epochs_to_run > 0:\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    try:\n",
        "        for epoch in range(start_epoch, epochs_to_run + 1):\n",
        "            epoch_start_time = time.time()\n",
        "            # Train one epoch\n",
        "            train(model, train_dataloader, epoch, criterion, optimizer, config.clip, dataset.vocab_size)\n",
        "\n",
        "            # Validate\n",
        "            val_loss, val_perplexity = evaluate(model, validation_dataloader, criterion, dataset.vocab_size)\n",
        "            train_loss, train_perplexity = evaluate(model, train_dataloader, criterion, dataset.vocab_size) # Eval on train too for comparison\n",
        "\n",
        "            # Store metrics for plotting\n",
        "            train_losses.append(train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "            train_perplexities.append(train_perplexity)\n",
        "            val_perplexities.append(val_perplexity)\n",
        "\n",
        "            print('-' * 89)\n",
        "            print(f'| end of epoch {epoch:3d} | time: {(time.time() - epoch_start_time):5.2f}s | '\n",
        "                  f'train loss {train_loss:5.2f} | train ppl {train_perplexity:8.2f} | '\n",
        "                  f'valid loss {val_loss:5.2f} | valid ppl {val_perplexity:8.2f}')\n",
        "            print('-' * 89)\n",
        "\n",
        "            # Save the model if validation perplexity improves\n",
        "            if val_perplexity < best_val_perplexity:\n",
        "                best_val_perplexity = val_perplexity\n",
        "                torch.save(model.state_dict(), config.save_path)\n",
        "                print(f\"** Saved best model (Epoch {epoch}) to {config.save_path} with Val PPL: {best_val_perplexity:.2f} **\")\n",
        "\n",
        "            # Optional Learning Rate Scheduling Step\n",
        "            # if scheduler: scheduler.step()\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('-' * 89)\n",
        "        print('Exiting training early via KeyboardInterrupt')\n",
        "        # Save the current model state if interrupted\n",
        "        interrupted_path = f\"{config.save_path}_interrupted_epoch{epoch}.pt\"\n",
        "        torch.save(model.state_dict(), interrupted_path)\n",
        "        print(f\"Saved interrupted model state to {interrupted_path}\")\n",
        "\n",
        "    # Load the best model found during training for final evaluation/generation\n",
        "    if os.path.exists(config.save_path):\n",
        "         print(f\"\\nLoading best model from {config.save_path} for final steps.\")\n",
        "         model.load_state_dict(torch.load(config.save_path, map_location=device))\n",
        "    else:\n",
        "         print(\"\\nWarning: No best model checkpoint saved (or training was skipped/interrupted early). Using final/loaded model state.\")\n",
        "\n",
        "#@title 16. Plot Training & Validation Metrics\n",
        "\n",
        "# Check if training occurred\n",
        "if train_losses and val_losses:\n",
        "    epochs_range = range(start_epoch, start_epoch + len(train_losses))\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, train_losses, label='Training Loss')\n",
        "    plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss (Cross Entropy)')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, train_perplexities, label='Training Perplexity')\n",
        "    plt.plot(epochs_range, val_perplexities, label='Validation Perplexity')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Perplexity')\n",
        "    plt.title('Training and Validation Perplexity')\n",
        "    # Optionally set y-axis limits for perplexity if it starts very high\n",
        "    # plt.ylim(0, max(val_perplexities)*1.1 if val_perplexities else 500) # Example limit\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"\\nSkipping metric plotting as no training was performed (model loaded or zero epochs).\")\n",
        "\n",
        "#@title 17. Final Evaluation and Generation\n",
        "print(\"\\n--- Final Evaluation on Validation Set ---\")\n",
        "# Ensure validation dataloader is ready\n",
        "if 'validation_dataloader' not in locals():\n",
        "     validation_sampler = SubsetRandomSampler(val_indices)\n",
        "     validation_dataloader = DataLoader(dataset, batch_size=config.batch_size, sampler=validation_sampler, drop_last=True)\n",
        "\n",
        "# Load best model if it exists and wasn't loaded already\n",
        "if not config.load_path and os.path.exists(config.save_path):\n",
        "    print(f\"Loading best model from {config.save_path} for final evaluation.\")\n",
        "    model.load_state_dict(torch.load(config.save_path, map_location=device))\n",
        "elif config.load_path:\n",
        "     print(\"Using the pre-loaded model for final evaluation.\")\n",
        "else:\n",
        "     print(\"Using the model state after last training epoch/interruption for final evaluation.\")\n",
        "\n",
        "\n",
        "final_val_loss, final_val_perplexity = evaluate(model, validation_dataloader, criterion, dataset.vocab_size)\n",
        "print(f\"Final Perplexity on Validation Data: {final_val_perplexity:.2f} (Loss: {final_val_loss:.2f})\")\n",
        "\n",
        "# --- Text Generation Comparison ---\n",
        "print(\"\\n--- Text Generation Samples ---\")\n",
        "print(f\"Prime Text: '{config.prime_text}'\")\n",
        "\n",
        "print(f\"\\nGenerating {config.num_samples} sample(s) with Temperature Sampling (Temp: {config.temperature}):\")\n",
        "for i in range(config.num_samples):\n",
        "    print(f\"\\n--- Sample {i+1} ---\")\n",
        "    generated_temp = generate_text_temperature(model, config.prime_text, config.generate_len, config.temperature, tokenizer, device)\n",
        "    print(generated_temp)\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "if config.beam_width > 0:\n",
        "    print(f\"\\nGenerating with Beam Search (Beam Width: {config.beam_width}):\")\n",
        "    # Beam search is deterministic, so one sample is usually enough to show its output style\n",
        "    print(f\"\\n--- Sample 1 ---\")\n",
        "    generated_beam = generate_text_beam_search(model, config.prime_text, config.generate_len, config.beam_width, tokenizer, device)\n",
        "    print(generated_beam)\n",
        "    print(\"-\" * 40)\n",
        "else:\n",
        "    print(\"\\nBeam search disabled (beam_width=0).\")"
      ]
    }
  ]
}